{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Polygenic score and imputation accuracy from low-pass sequencing in diverse population","text":"<p>This documentation provides comprehensive information on the code, data, and methods used in the article.</p>"},{"location":"#study-summary-comparing-genotyping-arrays-and-low-pass-wgs","title":"Study Summary: Comparing Genotyping Arrays and Low-Pass WGS","text":"<p>Traditional GWAS and PGS studies use SNP arrays with genotype imputation, but low-pass whole-genome sequencing (lpWGS) is emerging as a strong alternative.</p>"},{"location":"#study-design","title":"Study Design","text":"<ul> <li>Compared: 8 genotyping arrays vs. 6 lpWGS coverage levels (0.5\u00d7 to 2\u00d7)</li> <li>Population: 2,504 individuals from the 1000 Genomes Project</li> <li>Methods: Applied 10-fold cross-validation to perform genotype imputation and evaluate polygenic scores (PGS) across 4 traits. Results were summarized and assessed for performance.</li> </ul>"},{"location":"#key-findings","title":"Key Findings","text":"<ul> <li>lpWGS matched population-optimized arrays in imputation and PGS accuracy</li> <li>lpWGS outperformed arrays in underrepresented populations</li> <li>lpWGS was superior for rare and low-frequency variants</li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<p>Low-pass WGS is a flexible and powerful alternative to genotyping arrays, especially valuable for studies involving diverse or underrepresented populations.</p>"},{"location":"#analytical-pipeline-summary","title":"Analytical Pipeline Summary","text":"Figure 1: Overview of the analytical pipeline. A) 10-fold cross-imputation approach; (1) 10% of the samples are downsampled (BAM files) or filtered to retain only array variants (VCF files) to generate pseudo LPS and pseudo array data; (2) these data are imputed using the remaining 90% of the samples as the reference panel; (3) the imputed data from all batches are combined and then split by population; (4) performance is evaluated using high-coverage genotyping data as the ground truth. B) Data generation and imputation pipeline for LPS and SNP array data.   <p>This study analyzes data from 2,504 unrelated individuals in the 1000 Genomes Project<sup>1</sup>, re-sequenced at high coverage (30x) by the New York Genome Center (1KGPHC). Two main data sources are utilized:</p> <ul> <li>Mapped sequence data (CRAM format)</li> <li>Phased variant data (VCF format)</li> </ul>"},{"location":"#processes","title":"Processes","text":"<ol> <li>Processing data:<ul> <li>Cross-Validation Framework: A 10-fold stratified cross-validation ensures balanced population representation for imputation testing.</li> <li>Variant Filtering: VCF files are filtered to improve imputation accuracy.</li> <li>Data Simulation: Low-pass sequencing and eight SNP arrays data are simulated from high-coverage data.</li> </ul> </li> <li>Genotype Imputation: <ul> <li>lpWGS: GLIMPSE2 is used for lpWGS imputation.</li> <li>SNP arrays: undergo phasing with SHAPEIT5 and imputation with Minimac4.</li> </ul> </li> <li>Evaluation:<ul> <li>Restructure imputed data: Imputed data is merged by population</li> <li>lpWGS performance: compared to 30x WGS to assess accuracy and coverage performance, followed by visualization.</li> <li>PRS performance: We calculated PRS and compared it to 30\u00d7 WGS to assess PRS performance and visualize the results.</li> </ul> </li> </ol>"},{"location":"#appendix","title":"Appendix","text":"<ul> <li>Available data: Information on the datasets used in this study.</li> <li>About: Acknowledging contributions and support.</li> </ul> <ol> <li> <p>Marta Byrska-Bishop, Uday S Evani, Xuefang Zhao, Anna O Basile, Haley J Abel, Allison A Regier, Andr\u00e9 Corvelo, Wayne E Clarke, Rajeeva Musunuri, Kshithija Nagulapalli, and others. High-coverage whole-genome sequencing of the expanded 1000 genomes project cohort including 602 trios. Cell, 185(18):3426\u20133440, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"about/","title":"About","text":""},{"location":"about/#acknowledging","title":"Acknowledging","text":"<p>We would like to express our sincere gratitude to KTest Company, a biotechnology firm in Vietnam specializing in molecular biology, genetics, and bioinformatics solutions. Their generous support and valuable contributions have been instrumental in the successful completion of this research.</p>"},{"location":"about/#authors","title":"Authors","text":"Name Affiliation Email ORCID Author A Institution A authora@example.com 0000-0000-0000-0000 Author B Institution B authorb@example.com 0000-0000-0000-0000 Author C<sup>1</sup> Institution C authorc@example.com 0000-0000-0000-0000 Author D Institution D authord@example.com 0000-0000-0000-0000"},{"location":"about/#contact-us","title":"Contact Us","text":"<p>For any inquiries regarding this research, please contact the corresponding author:</p> <p>Author C Email: [example@email.com]</p> <ol> <li> <p> Corresponding author\u00a0\u21a9</p> </li> </ol>"},{"location":"available_data/","title":"Available data","text":""},{"location":"available_data/#license","title":"License","text":"<p>This dataset is released under the CC0 1.0 Universal (Public Domain Dedication).</p> <p>You are free to copy, modify, distribute, and use the data for any purpose, even commercially, without asking permission.</p> <p>No attribution is required, but citation is appreciated if you find this dataset useful.</p>"},{"location":"available_data/#disclosing-data","title":"Disclosing data","text":"Process Step Input Output Processing data Cross-Validation Framework - Samples list of batch - 2504 samples list - Population meta Variant Filtering - 3202 samples 1KGP<sup>1</sup> - 2504 samples list - Raw imputation panel Data Simulation - SNP-array pos data<sup>2</sup>- Samples list of batch- Raw imputation panel- GRCh38/hg38- URL metadata - Pseudo array VCFs - Downsampling VCFs Genotype Imputation lpWGS  imputation - Samples list of batch- Phasing reference- Raw imputation panel- Downsampling VCFs - lpWGS VCF files SNP arrays imputation - Samples list of batch- Phasing reference- Raw imputation panel- Pseudo array VCFs - SNP-array VCF files Evaluation Restructure imputed data - lpWGS VCF files- SNP-array VCF files- Population meta - Raw imputation panel - restructed lpWGS VCFs- restructed SNP-array VCFs- True VCFs lpWGS performance - restructed lpWGS VCFs- restructed SNP-array VCFs- True VCFs - LPS-arrays evaluation output PRS performance - restructed lpWGS VCFs- restructed SNP-array VCFs- True VCFs - Raw PRS scores- Percentile PRS scores- Visualized figures- Visualized tables <ol> <li> <p>Marta Byrska-Bishop, Uday S Evani, Xuefang Zhao, Anna O Basile, Haley J Abel, Allison A Regier, Andr\u00e9 Corvelo, Wayne E Clarke, Rajeeva Musunuri, Kshithija Nagulapalli, and others. High-coverage whole-genome sequencing of the expanded 1000 genomes project cohort including 602 trios. Cell, 185(18):3426\u20133440, 2022.\u00a0\u21a9</p> </li> <li> <p>Dat Thanh Nguyen, Trang TH Tran, Mai Hoang Tran, Khai Tran, Duy Pham, Nguyen Thuy Duong, Quan Nguyen, and Nam S Vo. A comprehensive evaluation of polygenic score and genotype imputation performances of human snp arrays in diverse populations. Scientific Reports, 12(1):17556, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"evaluation/lps_performance/","title":"LPS evaluation","text":""},{"location":"evaluation/lps_performance/#evaluation-methods","title":"Evaluation methods","text":"<p>To assess imputation performance, two key metrics are used: Imputation Accuracy and Imputation Coverage. These metrics quantify the quality and completeness of imputed genetic variants, and are calculated per chromosome across all autosomes.</p> Metric Description Purpose Imputation Accuracy Mean \\(r^2\\) of sites within a bin Measures how well imputed values match true genotypes Imputation Coverage Proportion of variants with \\(r^2 \\geq 0.8\\) in a bin Assesses the proportion of high-confidence imputations"},{"location":"evaluation/lps_performance/#evaluation-process","title":"Evaluation process","text":"<p>Input data</p> <ul> <li>restructed lpWGS VCFs</li> <li>restructed SNP-array VCFs</li> <li>True VCFs</li> </ul> Imputation AccuracyImputation coverage <p>Code</p> <pre><code>    compute_MAF.sh chr${i}_${pop}_true.vcf.gz maf.txt\n\n    run_evaluate.py --true_vcf chr${i}_${pop}_true.vcf.gz \\\n                    --imputed_vcf ${imputed_vcf} \\\n                    --af maf.txt \\\n                    --out_snp_wise chr${i}_${lps_cov}_${pop}_snp_wise.acc\n</code></pre> <ul> <li>compute_MAF.sh: Retrieve MAF values from true VCF files</li> <li>run_evaluate.py: Evaluation by using SNP-wise matrix </li> </ul> <p>Code</p> <pre><code>  get_coverage.py --input ${res_snp_wise} \\\n                  --cov perbin_${res_snp_wise}_cov.txt \\\n                  --acc perbin_${res_snp_wise}_mean_r2.txt \n</code></pre> <ul> <li>get_coverage.py: Evaluation using Imputation coverage matrix  </li> </ul> <p>Output</p> <p>Evaluation process output:</p> LPS Pseudo array SNP-wise accuracy lps_all_acc.txt array_all_acc.txt Imputation accuracy lps_all_cov.txt array_all_cov.txt"},{"location":"evaluation/prs_performace/","title":"PRS evaluation","text":"<p>PGS performance was evaluated using two main metrics:</p> <ol> <li> <p>PGS Correlation    Pearson\u2019s correlation between PGS derived from imputed SNP array data and PGS from whole-genome sequencing (WGS).</p> </li> <li> <p>ADPR (Absolute Difference in Percentile Ranking)    The absolute difference in percentile ranking between PGS from array-imputed data and the WGS-derived gold standard.</p> </li> </ol> <p>These evaluations were conducted across multiple p-value thresholds to ensure unbiased comparison, based on the method from Nguyen et al., 2022<sup>1</sup>.</p> <p>PRS formula</p> <p>For an individual \\(i\\), the polygenic score at p-value threshold \\(P_{T}\\) is calculated as:</p> \\[ PGS_i(P_T) = \\sum_{j=1}^{M} {1}_{\\{P_j &lt; P_T\\}} \\, x_{ij} \\, \\hat{\\beta}_j \\] <ul> <li>\\(P_T\\) : p-value threshold</li> <li>\\(M\\) : number of SNPs after clumping</li> <li>\\(x_{ij}\\) : allele count for SNP \\(j\\) in individual \\(i\\)</li> <li>\\(\\hat{\\beta}_j\\) : marginal effect size from GWAS for SNP \\(j\\)</li> <li>\\(1_{\\{P_j &lt; P_T\\}}\\) : indicator function for p-value filtering</li> </ul> PGS correlationADPR <ol> <li> <p>Dat Thanh Nguyen, Trang TH Tran, Mai Hoang Tran, Khai Tran, Duy Pham, Nguyen Thuy Duong, Quan Nguyen, and Nam S Vo. A comprehensive evaluation of polygenic score and genotype imputation performances of human snp arrays in diverse populations. Scientific Reports, 12(1):17556, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"evaluation/prs_performace/#pgs-correlation","title":"PGS correlation","text":"<ul> <li>It is the Pearson correlation coefficient between imputed and true sets of raw PGS values computed for the same individuals.</li> <li>Interpretation: Measures how similar the PGS values are in scale and ranking across two methods.</li> </ul>"},{"location":"evaluation/prs_performace/#absolute-difference-in-percentile-ranking-adpr","title":"Absolute Difference in Percentile Ranking (ADPR)","text":"<ul> <li>It is the average absolute difference in percentile rank of each individual between imputed and true sets of PGS.</li> <li> <p>Formula:</p> \\[     \\text{ADPR} = \\frac{1}{N} \\sum_{i=1}^{N} \\left| \\text{percentile}_i^{(A)} - \\text{percentile}_i^{(B)} \\right| \\] <ul> <li><code>N</code> is the number of individuals</li> <li><code>percentile_i_A</code> and <code>percentile_i_B</code> are the percentile ranks of individual <code>i</code> in each PGS imputed and true distribution</li> </ul> </li> </ul>"},{"location":"evaluation/prs_processing/","title":"PRS processing","text":"<p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> <li>plink v1.90</li> <li>PRSice-2 v2.3.3</li> </ul> <p>Input data</p> <ul> <li>restructed lpWGS VCFs</li> <li>restructed SNP-array VCFs</li> <li>True VCFs</li> </ul> PRS processing workflow <p></p>"},{"location":"evaluation/prs_processing/#correct-sample-name","title":"Correct sample name","text":"<p>Ensure that sample names do not contain underscores, as these may be introduced during the merging of imputed VCF files. In such cases, the filename used during merging may be incorporated into the sample name to maintain uniqueness across datasets.</p> <p>Code</p> <pre><code>set -ue\n\nVCF_FILES=$1\n\n# Get correct name\nbcftools query -l $VCF_FILES &gt; sample_name.txt\nsed -E 's/^(([^_]+)_.*)/\\1\\t\\2/g' sample_name.txt &gt; new_name.txt\n\n# Check whether origin has correct name\ncol_num=`awk -F'\\t' '{print NF}' new_name.txt | head -n 1`\nchecker=`if (( $col_num==1 )); then echo 1; else echo 0; fi`\n\nfile_name=$(basename $VCF_FILES .vcf.gz)\n\nif (( $checker )); then\n    echo \"Create symlink\"\n    ln -s $VCF_FILES ${file_name}.vcf.gz\nelse\n    # Replace old name by new one\n    echo \"Replace by new name\"\n    bcftools reheader -s new_name.txt -o ${file_name}_correct_name.vcf.gz $VCF_FILES\nfi\n\nbcftools index ${file_name}_correct_name.vcf.gz\n</code></pre>"},{"location":"evaluation/prs_processing/#concatenate-vcf-files","title":"Concatenate VCF files","text":"<p>Concatenate autosome VCF files have same prefix (Array name/ lowpass coverage).</p> <p>Code</p> <pre><code>set -ue\n\nPREFIX=$1     #PREFIX=\"AMR-Axiom_JAPONICA\"\nVCF_FOLDER=$2    #VCF_FOLDER=\"/path/to/vcf_files\"\n\n# List VCF_FILES\nVCF_FILES=$(ls ${VCF_FOLDER}/${PREFIX}_chr*.vcf.gz)\n\nbcftools concat -Oz -o ${PREFIX}_concat.vcf.gz ${VCF_FILES}\nbcftools index ${PREFIX}_concat.vcf.gz\n</code></pre>"},{"location":"evaluation/prs_processing/#annotate-vcf-files","title":"Annotate VCF files","text":"<p>Code</p> <pre><code>set -eu\n\nVCF_FILES=$1  # VCF_FILES=\"AMR-Axiom_UKB_WCSG_concat.vcf.gz\"\nREF_VCF=$2   # REF_VCF=\"00-All.vcf.gz\"\n\nfile_name=$(basename $VCF_FILES .vcf.gz)\n\n## Make sure the format chromosome names is numeric\nbcftools view ${VCF_FILES} |                 sed 's/chr//g' |                 bgzip &gt; tem.vcf.gz\nbcftools index tem.vcf.gz\n\n## Annotate the VCF with reference VCF\nbcftools annotate -a ${REF_VCF} -c ID -o ${file_name}_anno.vcf.gz tem.vcf.gz\nbcftools index ${file_name}_anno.vcf.gz\n\n## Remove temporary files\nrm tem.vcf.gz tem.vcf.gz.csi\n</code></pre>"},{"location":"evaluation/prs_processing/#convert-vcf-files-to-bed-files","title":"Convert VCF files to BED files","text":"<p>Code</p> <pre><code>set -eu\n\nVCF_FILES=$1  # VCF_FILES=\"AMR-Axiom_UKB_WCSG_anno.vcf.gz\"\n\nprefix=$(basename $VCF_FILES _anno.vcf.gz)\n\nplink   --vcf ${VCF_FILES} \\\n        --make-bed  \\\n        --const-fid \\\n        --out ${prefix}       \\\n        --threads 2       \\\n        --memory 128000\n</code></pre>"},{"location":"evaluation/prs_processing/#qc-vcf-files","title":"QC VCF files","text":"<p>Code</p> <pre><code>PREFIX=$1  # e.g. AMR-Axiom_UKB_WCSG\n\nplink     --bfile ${PREFIX}     \\\n          --maf 0.0001     \\\n          --hwe 1e-6     \\\n          --geno 0.01     \\\n          --mind 0.01     \\\n          --write-snplist     \\\n          --make-just-fam     \\\n          --memory 128000     \\\n          --out ${PREFIX}.QC\n</code></pre>"},{"location":"evaluation/prs_processing/#deduplicate","title":"Deduplicate","text":"<p>Code</p> <pre><code>set -ue\n\nPREFIX=$1  # e.g. AMR-Axiom_UKB_WCSG\n\n## List duplicated records\nRscript LIST_NO_DUPLICATE.R ${PREFIX}.QC.snplist ${PREFIX}.nodup\n\n## Deduplicate\nplink         --bfile ${PREFIX}         \\\n              --threads 2         \\\n              --make-bed         \\\n              --keep ${PREFIX}.QC.fam         \\\n              --out ${PREFIX}.dedup         \\\n              --extract ${PREFIX}.nodup         \\\n              --memory 128000\n</code></pre> <ul> <li>LIST_NO_DUPLICATE.R</li> </ul>"},{"location":"evaluation/prs_processing/#get-raw-prs-score","title":"Get raw PRS score","text":"<p>Code</p> <pre><code>TARGET_FILE=$1      # e.g. AMR-Axiom_UKB_WCSG.dedup\nBASE_FILE=$2        # e.g. GIANT_BMI.QCed.gz\nOUT_PREFIX=$3       # e.g. AMR-Axiom_UKB_WCSG_bmi\nLD_TRUE=$4          # e.g. AMR-null.dedup\n\nPRSice     --prsice /src/PRSice-2_v2.3.3/PRSice_linux     \\\n           --base ${BASE_FILE}     \\\n           --target ${TARGET_FILE}     \\\n           --ld ${LD_TRUE}     \\\n           --out ${OUT_PREFIX}     \\\n           --binary-target F     \\\n           --bar-levels 0.00000005,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,0.2,0.3,0.5,1 \\\n           --fastscore     \\\n           --a1 A1     \\\n           --a2 A2     \\\n           --beta      \\\n           --bp BP     \\\n           --chr CHR     \\\n           --pvalue P     \\\n           --snp SNP     \\\n           --stat BETA     \\\n           --clump-kb 250kb     \\\n           --clump-p 1     \\\n           --clump-r2 0.1     \\\n           --ultra     \\\n           --no-regress     \\\n           --score sum     \\\n           --thread 1\n</code></pre>"},{"location":"evaluation/prs_processing/#prepare-percentile-prs-scores","title":"Prepare percentile PRS scores","text":"<p>Code</p> <pre><code>...\n</code></pre> <p>Output data</p> <ul> <li>Raw PRS scores</li> <li>Percentile PRS scores</li> </ul>"},{"location":"evaluation/restructure_imputed_data/","title":"Restructure imputed data","text":"<p>After the imputation process, the data must be stratified by the five superpopulations (EUR, EAS, AMR, AFR, SAS) to enable population-specific evaluation.</p> <p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> </ul> Low-pass sequencing dataPseudo SNP Arrays dataTrue VCFs"},{"location":"evaluation/restructure_imputed_data/#low-pass-sequencing-data","title":"Low-pass sequencing data","text":"<p>Input data</p> <ul> <li>lpWGS VCF files</li> <li>Population meta</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#merge-imputed-data","title":"Merge imputed data","text":"<p>Code</p> <p>Merge samples from imputed batches <pre><code>set -ue\n\n## Input\nCHR=$1\nIMPUTED_FOLDER=$2\nLPS_COV=$3\nTOTAL_SAMPLE=$4\n\n## Merge batches\nmerge_batches.sh ${IMPUTED_FOLDER} \"chr${CHR}_${LPS_COV}\" \"tem.vcf.gz\"\nrename_samples.sh tem.vcf.gz \"_${LPS_COV}\" chr${CHR}_${LPS_COV}_merged_all.vcf.gz ${TOTAL_SAMPLE}\nrm tem.vcf.gz\n</code></pre></p> <ul> <li>merge_batches.sh</li> <li>rename_samples.sh</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#restruct-imputed-lps-vcfs","title":"Restruct imputed LPS VCFs","text":"<p>Code</p> <p>Imputed VCFs is restructured by supperpopulation <pre><code>set -ue\n\nPOPULATION_META=$1\nPOP_NAME=$2\nMERGED_VCF=$3\nCHR=$4\nLPS_COV=$5\n\n## Get sample list for the specified population\nawk -F'\\t' -v pop_name=${POP_NAME} 'NR!=1 &amp;&amp; $6==pop_name {print $1}' ${POPULATION_META} &gt; ${POP_NAME}_sample_list.txt\n\n# Filter the merged VCF for the specified population\nbcftools view -S ${POP_NAME}_sample_list.txt ${MERGED_VCF} | bgzip &gt; chr${CHR}_${LPS_COV}_${POP_NAME}_imputed.vcf.gz\n</code></pre></p> <p>Output data</p> <ul> <li>restructed lpWGS VCFs</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#pseudo-snp-arrays-data","title":"Pseudo SNP Arrays data","text":"<p>Input data</p> <ul> <li>SNP-array VCF files</li> <li>Population meta</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#merge-imputed-data_1","title":"Merge imputed data","text":"<p>Code</p> <pre><code>set -ue\n\n## Input\nCHR=$1\nIMPUTED_FOLDER=$2\nLPS_COV=$3\n\nmerge_array_batches.sh ${IMPUTED_FOLDER} \"${LPS_COV}_chr${CHR}\" \"chr${CHR}_${LPS_COV}_merged_all.vcf.gz\"\n</code></pre> <ul> <li>merge_array_batches.sh</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#restruct-imputed-pseudo-array-vcfs","title":"Restruct imputed Pseudo-array VCFs","text":"<p>Code</p> <pre><code>set -ue\n\nPOPULATION_META=$1\nPOP_NAME=$2\nMERGED_VCF=$3\nCHR=$4\nLPS_COV=$5\n\n## Get sample list for the specified population\nawk -F'\\t' -v pop_name=${POP_NAME} 'NR!=1 &amp;&amp; $6==pop_name {print $1}' ${POPULATION_META} &gt; ${POP_NAME}_sample_list.txt\n\n## Filter the merged VCF for the specified population\nbcftools view -S ${POP_NAME}_sample_list.txt ${MERGED_VCF} | bgzip &gt; chr${CHR}_${LPS_COV}_${POP_NAME}_imputed.vcf.gz\n</code></pre> <p>Output data</p> <ul> <li>restructed SNP-array VCFs</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#prepare-true-vcfs-according-supperpopulation","title":"Prepare true VCFs according supperpopulation","text":"<p>Preparation of true VCFs involves extracting the corresponding samples from the reference panel for each of the five superpopulations.</p> <p>Input data</p> <ul> <li>Imputation panel</li> <li>Population meta</li> </ul>"},{"location":"evaluation/restructure_imputed_data/#processing","title":"Processing","text":"<p>Code</p> <pre><code>set -ue\n\n## Input\nPOPULATION_META=$1\nPOP_NAME=$2\nTRUE_VCF=$3\nCHR=$4\n\n## Get sample list for the specified population\nawk -F'\\t' -v pop_name=${POP_NAME} 'NR!=1 &amp;&amp; $6==pop_name {print $1}' ${POPULATION_META} &gt; ${POP_NAME}_sample_list.txt\n\n## VCF_true_population_slipt\nbcftools view -S ${POP_NAME}_sample_list.txt ${TRUE_VCF} | bgzip &gt; chr${CHR}_${POP_NAME}_true.vcf.gz\n</code></pre> <p>Output data</p> <ul> <li>True VCFs being collected by supperpopulation</li> </ul>"},{"location":"evaluation/jupyter-files/01.imp_acc/","title":"Accuracy performance","text":"<p>Requirements</p> <ul> <li>python&gt;=3.11</li> <li>pandas==2.3.0</li> <li>seaborn==0.13.2</li> <li>matplotlib==3.10.3</li> <li>numpy==2.3.1</li> </ul> <p>Input data</p> <ul> <li>LPS-arrays evaluation output</li> </ul> In\u00a0[1]: Copied! <pre>## Import Python packages\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> ## Import Python packages import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>## Input variables\nraw_lps_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/lps_all_acc.txt'\nraw_arr_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/array_all_acc.txt'\n\noutput_figs_folder = '../../../evaluation/downstream/out_figs'\noutput_table_folder = '../../../evaluation/downstream/out_tables/accuracy'\n</pre> ## Input variables raw_lps_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/lps_all_acc.txt' raw_arr_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/array_all_acc.txt'  output_figs_folder = '../../../evaluation/downstream/out_figs' output_table_folder = '../../../evaluation/downstream/out_tables/accuracy' In\u00a0[3]: Copied! <pre>def get_lps(s):\n    \"\"\"\n    Extracts population and model identifier from a file name (LPS-style).\n    Example input: \"result_model1_array1_EUR.txt\"\n    Output: \"EUR_model1\"\n    \"\"\"\n    t = s.split('_')\n    return f'{t[3].upper()}_{t[2]}'\n\ndef get_array(s):\n    \"\"\"\n    Extracts array name from a file name (Array-style).\n    Example input: \"result_model1_PMRA_EUR.txt\"\n    Output: \"PMRA\"\n    \"\"\"\n    t = s.split('_')\n    return f'{t[2]}'\n</pre> def get_lps(s):     \"\"\"     Extracts population and model identifier from a file name (LPS-style).     Example input: \"result_model1_array1_EUR.txt\"     Output: \"EUR_model1\"     \"\"\"     t = s.split('_')     return f'{t[3].upper()}_{t[2]}'  def get_array(s):     \"\"\"     Extracts array name from a file name (Array-style).     Example input: \"result_model1_PMRA_EUR.txt\"     Output: \"PMRA\"     \"\"\"     t = s.split('_')     return f'{t[2]}' In\u00a0[4]: Copied! <pre>def process_perbin(path, array=None):\n    \"\"\"\n    Process a per-bin results file and return a standardized long-format DataFrame.\n\n    Parameters:\n    - path (str): Path to the tab-separated result file.\n    - array (bool): True if the file is from an array dataset, False (default) if from an LPS dataset.\n\n    Returns:\n    - pd.DataFrame: Long-format data with columns ['file', 'bins', 'value', 'array', 'pop', 'type']\n    \"\"\"\n    # Read the file into a DataFrame with no header\n    df = pd.read_csv(path, header=None, sep='\\t')\n\n    # Rename columns: 'file' for the file identifier, followed by bins\n    df.columns = [\"file\", \"(0\u20130.01]\", \"(0.01\u20130.05]\", \"(0.05\u20130.5]\", \"(0.01\u20130.5]\"]\n\n    # Melt the DataFrame to long format: one row per bin per file\n    df = pd.melt(df, id_vars=['file'], var_name='bins')\n\n    if array:\n        # Standardize array naming from various vendor-specific labels\n        df['file'] = df['file'].replace(regex={\n            'Axiom_PMRA': 'PMRA',\n            'Axiom_JAPONICA': 'JAPONICA',\n            'infinium-omni2.5.v1.5': 'OMNI2.5',\n            'cytosnp-850k-v1.2': 'CYTOSNP',\n            'global-screening-array-v.3': 'GSA',\n            'infinium-omni5-v1.2': 'OMNI5',\n            'Axiom_PMDA': 'PMDA',\n            'Axiom_UKB_WCSG': 'UKB-WCSG'\n        })\n\n        # Extract array name and population code\n        df['array'] = [get_array(i) for i in df['file']]\n        df['pop'] = [i.split('_')[3] for i in df['file']]\n\n        # Normalize naming for UKB_WCSG\n        df['array'] = df['array'].replace(regex={\n            'UKB-WCSG': 'UKB_WCSG'\n        })\n\n        df['type'] = 'array'\n    else:\n        # Extract composite identifier and population for LPS\n        df['array'] = [get_lps(i) for i in df['file']]\n        df['pop'] = [i.split('_')[4] for i in df['file']]\n        df['type'] = 'lps'\n\n    return df \n    \n</pre> def process_perbin(path, array=None):     \"\"\"     Process a per-bin results file and return a standardized long-format DataFrame.      Parameters:     - path (str): Path to the tab-separated result file.     - array (bool): True if the file is from an array dataset, False (default) if from an LPS dataset.      Returns:     - pd.DataFrame: Long-format data with columns ['file', 'bins', 'value', 'array', 'pop', 'type']     \"\"\"     # Read the file into a DataFrame with no header     df = pd.read_csv(path, header=None, sep='\\t')      # Rename columns: 'file' for the file identifier, followed by bins     df.columns = [\"file\", \"(0\u20130.01]\", \"(0.01\u20130.05]\", \"(0.05\u20130.5]\", \"(0.01\u20130.5]\"]      # Melt the DataFrame to long format: one row per bin per file     df = pd.melt(df, id_vars=['file'], var_name='bins')      if array:         # Standardize array naming from various vendor-specific labels         df['file'] = df['file'].replace(regex={             'Axiom_PMRA': 'PMRA',             'Axiom_JAPONICA': 'JAPONICA',             'infinium-omni2.5.v1.5': 'OMNI2.5',             'cytosnp-850k-v1.2': 'CYTOSNP',             'global-screening-array-v.3': 'GSA',             'infinium-omni5-v1.2': 'OMNI5',             'Axiom_PMDA': 'PMDA',             'Axiom_UKB_WCSG': 'UKB-WCSG'         })          # Extract array name and population code         df['array'] = [get_array(i) for i in df['file']]         df['pop'] = [i.split('_')[3] for i in df['file']]          # Normalize naming for UKB_WCSG         df['array'] = df['array'].replace(regex={             'UKB-WCSG': 'UKB_WCSG'         })          df['type'] = 'array'     else:         # Extract composite identifier and population for LPS         df['array'] = [get_lps(i) for i in df['file']]         df['pop'] = [i.split('_')[4] for i in df['file']]         df['type'] = 'lps'      return df       In\u00a0[5]: Copied! <pre># Process both LPS and array results\nlps_res = process_perbin(raw_lps_res)\narray_res = process_perbin(raw_arr_res, array=True)\n</pre> # Process both LPS and array results lps_res = process_perbin(raw_lps_res) array_res = process_perbin(raw_arr_res, array=True) In\u00a0[6]: Copied! <pre># Combine both results into a single DataFrame\ndata = pd.concat([lps_res, array_res])\n</pre> # Combine both results into a single DataFrame data = pd.concat([lps_res, array_res]) <p>Show the first few rows of the merged dataset</p> In\u00a0[7]: Copied! <pre>data.head()\n</pre> data.head() Out[7]: file bins value array pop type 0 perbin_chr10_0.5_lps_AFR_snp_wise.acc_mean_r2.txt (0\u20130.01] 0.718050 LPS_0.5 AFR lps 1 perbin_chr10_0.5_lps_AMR_snp_wise.acc_mean_r2.txt (0\u20130.01] 0.805808 LPS_0.5 AMR lps 2 perbin_chr10_0.5_lps_EAS_snp_wise.acc_mean_r2.txt (0\u20130.01] 0.510027 LPS_0.5 EAS lps 3 perbin_chr10_0.5_lps_EUR_snp_wise.acc_mean_r2.txt (0\u20130.01] 0.663159 LPS_0.5 EUR lps 4 perbin_chr10_0.5_lps_SAS_snp_wise.acc_mean_r2.txt (0\u20130.01] 0.576910 LPS_0.5 SAS lps In\u00a0[8]: Copied! <pre>def do_filter_data(data, bins, pop):\n    \"\"\"\n    Filter data by MAF bin and population\n    \"\"\"\n    pick = (data['bins'] == bins) &amp; (data['pop'] == pop)\n    return data[pick]\n</pre> def do_filter_data(data, bins, pop):     \"\"\"     Filter data by MAF bin and population     \"\"\"     pick = (data['bins'] == bins) &amp; (data['pop'] == pop)     return data[pick] In\u00a0[9]: Copied! <pre># Desired array/LPS order for consistent plotting\ndesired_order = ['GSA', \n                'JAPONICA',\n                'UKB_WCSG',\n                'CYTOSNP', \n                'PMRA', \n                'PMDA',\n                'OMNI2.5', \n                'OMNI5',\n                'LPS_0.5',\n                'LPS_0.75',\n                'LPS_1.0',\n                'LPS_1.25',\n                'LPS_1.5',\n                'LPS_2.0'] \n</pre> # Desired array/LPS order for consistent plotting desired_order = ['GSA',                  'JAPONICA',                 'UKB_WCSG',                 'CYTOSNP',                  'PMRA',                  'PMDA',                 'OMNI2.5',                  'OMNI5',                 'LPS_0.5',                 'LPS_0.75',                 'LPS_1.0',                 'LPS_1.25',                 'LPS_1.5',                 'LPS_2.0']  In\u00a0[10]: Copied! <pre>def plot_1(data, bins, pop, axe):\n    \"\"\"\n    Generate a boxplot for one bin/pop combination on the given axis.\n    \"\"\"\n    filterd_data = do_filter_data(data, bins, pop)\n    #print(filterd_data.shape)\n    sns.boxplot(data=filterd_data, \n                y = 'array', \n                x = 'value', \n                hue='type', fill=False,\n                width=0.4,\n                order=desired_order,\n                fliersize= 0.5,\n                orient='h',\n                ax=axe)\n    threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()\n    #print(threshold)\n    axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)\n    axe.grid(axis='both', linewidth=0.5)\n    axe.legend().set_visible(False)\n</pre> def plot_1(data, bins, pop, axe):     \"\"\"     Generate a boxplot for one bin/pop combination on the given axis.     \"\"\"     filterd_data = do_filter_data(data, bins, pop)     #print(filterd_data.shape)     sns.boxplot(data=filterd_data,                  y = 'array',                  x = 'value',                  hue='type', fill=False,                 width=0.4,                 order=desired_order,                 fliersize= 0.5,                 orient='h',                 ax=axe)     threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()     #print(threshold)     axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)     axe.grid(axis='both', linewidth=0.5)     axe.legend().set_visible(False) In\u00a0[11]: Copied! <pre>def full_plot(data):\n    \"\"\"\n    Generate a full grid of plots, one per (bin, pop) combination.\n    \"\"\"\n    cols = data['bins'].unique()\n    rows = data['pop'].unique()\n    fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))\n\n    for i, x in enumerate(rows):\n        for j, y in enumerate(cols):\n            plot_1(data, y, x, axes[i,j])\n            \n            # Ticks\n            axes[i,j].set_xlim(0, 1.01)\n            axes[i,j].set_xticks(np.arange(0, 1.01, 0.2))\n            axes[i,j].set_ylabel(\"\")\n            axes[i,j].set_xlabel(\"\")\n            \n            if j != 0:\n                pass\n                \n            if i == 0:\n                # Title\n                axes[i,j].set_title(label=y, \n                            color='white', \n                            bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'), \n                            x=0.5, pad=12,\n                            fontdict={'fontsize':10})\n            if j == 0:\n                axes[i,j].set_ylabel(ylabel=x,\n                                                color='white',\n                                                labelpad = 12,\n                                                rotation = 'horizontal', \n                                                bbox=dict(facecolor='#b3b3b3', \n                                                          edgecolor='white', \n                                                          boxstyle='round,pad=0.6'),\n                                                fontdict={'fontsize':10})\n            \n    for line_num, line_axis in enumerate(axes):\n        for col_num, ax in enumerate(line_axis):\n            if col_num == 0:\n                continue\n            ax.set_ylim(line_axis[0].get_ylim()) # align axes\n            plt.setp(ax.get_yticklabels(), visible=False)\n                \n    \n    fig.tight_layout(rect=[0.02, 0.02, 1, 1])\n    plt.savefig(f'{output_figs_folder}/acc.pdf', dpi=300)\n</pre>  def full_plot(data):     \"\"\"     Generate a full grid of plots, one per (bin, pop) combination.     \"\"\"     cols = data['bins'].unique()     rows = data['pop'].unique()     fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))      for i, x in enumerate(rows):         for j, y in enumerate(cols):             plot_1(data, y, x, axes[i,j])                          # Ticks             axes[i,j].set_xlim(0, 1.01)             axes[i,j].set_xticks(np.arange(0, 1.01, 0.2))             axes[i,j].set_ylabel(\"\")             axes[i,j].set_xlabel(\"\")                          if j != 0:                 pass                              if i == 0:                 # Title                 axes[i,j].set_title(label=y,                              color='white',                              bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'),                              x=0.5, pad=12,                             fontdict={'fontsize':10})             if j == 0:                 axes[i,j].set_ylabel(ylabel=x,                                                 color='white',                                                 labelpad = 12,                                                 rotation = 'horizontal',                                                  bbox=dict(facecolor='#b3b3b3',                                                            edgecolor='white',                                                            boxstyle='round,pad=0.6'),                                                 fontdict={'fontsize':10})                  for line_num, line_axis in enumerate(axes):         for col_num, ax in enumerate(line_axis):             if col_num == 0:                 continue             ax.set_ylim(line_axis[0].get_ylim()) # align axes             plt.setp(ax.get_yticklabels(), visible=False)                           fig.tight_layout(rect=[0.02, 0.02, 1, 1])     plt.savefig(f'{output_figs_folder}/acc.pdf', dpi=300)  In\u00a0[12]: Copied! <pre>full_plot(data)\n</pre> full_plot(data) In\u00a0[13]: Copied! <pre>df = data\n# Compute group-wise mean and standard deviation\nsummary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(\n    mean=(\"value\", \"mean\"),\n    std=(\"value\", \"std\")\n).reset_index()\n\n# Format summary into \"mean \u00b1 std\" string\nsummary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)\n\n# Convert to a wide-format table: each population is a column\nresult = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()\n\n# Ensure arrays are ordered as defined\nresult[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True)\nresult = result.sort_values([\"bins\", \"array\"])\n\n# Export separate CSV files for each bin\nfor bin_val, df_bin in result.groupby(\"bins\"):\n    df_bin = df_bin.drop(columns=[\"bins\"])\n    df_bin = df_bin.rename(columns={'array': 'Array/LPS'})\n    df_bin.to_csv(f\"{output_table_folder}/mean_r2_summary_bin_{bin_val}.csv\", index=False)\n</pre> df = data # Compute group-wise mean and standard deviation summary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(     mean=(\"value\", \"mean\"),     std=(\"value\", \"std\") ).reset_index()  # Format summary into \"mean \u00b1 std\" string summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)  # Convert to a wide-format table: each population is a column result = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()  # Ensure arrays are ordered as defined result[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True) result = result.sort_values([\"bins\", \"array\"])  # Export separate CSV files for each bin for bin_val, df_bin in result.groupby(\"bins\"):     df_bin = df_bin.drop(columns=[\"bins\"])     df_bin = df_bin.rename(columns={'array': 'Array/LPS'})     df_bin.to_csv(f\"{output_table_folder}/mean_r2_summary_bin_{bin_val}.csv\", index=False) <p>Outputs</p> <ul> <li>LPS accuracy-evaluation tables</li> <li>LPS accuracy-evaluation plot</li> </ul>"},{"location":"evaluation/jupyter-files/01.imp_acc/#processing-raw-data","title":"Processing raw data\u00b6","text":""},{"location":"evaluation/jupyter-files/01.imp_acc/#plot-data","title":"Plot data\u00b6","text":""},{"location":"evaluation/jupyter-files/01.imp_acc/#data-exporter","title":"Data Exporter\u00b6","text":""},{"location":"evaluation/jupyter-files/02.imp_cov/","title":"Coverage performance","text":"<p>Requirements</p> <ul> <li>python&gt;=3.11</li> <li>pandas==2.3.0</li> <li>seaborn==0.13.2</li> <li>matplotlib==3.10.3</li> <li>numpy==2.3.1</li> </ul> <p>Input data</p> <ul> <li>LPS-arrays evaluation output</li> </ul> In\u00a0[1]: Copied! <pre>## Import Python packages\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> ## Import Python packages import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>## Input variables\nraw_lps_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/lps_all_cov.txt'\nraw_arr_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/array_all_cov.txt'\n\noutput_figs_folder = '../../../evaluation/downstream/out_figs'\noutput_table_folder = '../../../evaluation/downstream/out_tables/coverage'\n</pre> ## Input variables raw_lps_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/lps_all_cov.txt' raw_arr_res = '../../../evaluation/downstream/data/raw_snpwise_accuracy/array_all_cov.txt'  output_figs_folder = '../../../evaluation/downstream/out_figs' output_table_folder = '../../../evaluation/downstream/out_tables/coverage' In\u00a0[3]: Copied! <pre>def get_lps(s):\n    \"\"\"\n    Extracts population and model identifier from a file name (LPS-style).\n    Example input: \"result_model1_array1_EUR.txt\"\n    Output: \"EUR_model1\"\n    \"\"\"\n    t = s.split('_')\n    return f'{t[3].upper()}_{t[2]}'\n\ndef get_array(s):\n    \"\"\"\n    Extracts array name from a file name (Array-style).\n    Example input: \"result_model1_PMRA_EUR.txt\"\n    Output: \"PMRA\"\n    \"\"\"\n    t = s.split('_')\n    return f'{t[2]}'    \n</pre> def get_lps(s):     \"\"\"     Extracts population and model identifier from a file name (LPS-style).     Example input: \"result_model1_array1_EUR.txt\"     Output: \"EUR_model1\"     \"\"\"     t = s.split('_')     return f'{t[3].upper()}_{t[2]}'  def get_array(s):     \"\"\"     Extracts array name from a file name (Array-style).     Example input: \"result_model1_PMRA_EUR.txt\"     Output: \"PMRA\"     \"\"\"     t = s.split('_')     return f'{t[2]}'     In\u00a0[4]: Copied! <pre>def process_perbin(path, array=None):\n    \"\"\"\n    Process a per-bin results file and return a standardized long-format DataFrame.\n\n    Parameters:\n    - path (str): Path to the tab-separated result file.\n    - array (bool): True if the file is from an array dataset, False (default) if from an LPS dataset.\n\n    Returns:\n    - pd.DataFrame: Long-format data with columns ['file', 'bins', 'value', 'array', 'pop', 'type']\n    \"\"\"\n    # Read the file into a DataFrame with no header\n    df = pd.read_csv(path, header=None, sep='\\t')\n\n    # Rename columns: 'file' for the file identifier, followed by bins\n    df.columns = [\"file\", \"(0\u20130.01]\", \"(0.01\u20130.05]\", \"(0.05\u20130.5]\", \"(0.01\u20130.5]\"]\n\n    # Melt the DataFrame to long format: one row per bin per file\n    df = pd.melt(df, id_vars=['file'], var_name='bins')\n\n    if array:\n        # Standardize array naming from various vendor-specific labels\n        df['file'] = df['file'].replace(regex={\n            'Axiom_PMRA': 'PMRA',\n            'Axiom_JAPONICA': 'JAPONICA',\n            'infinium-omni2.5.v1.5': 'OMNI2.5',\n            'cytosnp-850k-v1.2': 'CYTOSNP',\n            'global-screening-array-v.3': 'GSA',\n            'infinium-omni5-v1.2': 'OMNI5',\n            'Axiom_PMDA': 'PMDA',\n            'Axiom_UKB_WCSG': 'UKB-WCSG'\n        })\n\n        # Extract array name and population code\n        df['array'] = [get_array(i) for i in df['file']]\n        df['pop'] = [i.split('_')[3] for i in df['file']]\n\n        # Normalize naming for UKB_WCSG\n        df['array'] = df['array'].replace(regex={\n            'UKB-WCSG': 'UKB_WCSG'\n        })\n\n        df['type'] = 'array'\n    else:\n        # Extract composite identifier and population for LPS\n        df['array'] = [get_lps(i) for i in df['file']]\n        df['pop'] = [i.split('_')[4] for i in df['file']]\n        df['type'] = 'lps'\n\n    return df \n</pre> def process_perbin(path, array=None):     \"\"\"     Process a per-bin results file and return a standardized long-format DataFrame.      Parameters:     - path (str): Path to the tab-separated result file.     - array (bool): True if the file is from an array dataset, False (default) if from an LPS dataset.      Returns:     - pd.DataFrame: Long-format data with columns ['file', 'bins', 'value', 'array', 'pop', 'type']     \"\"\"     # Read the file into a DataFrame with no header     df = pd.read_csv(path, header=None, sep='\\t')      # Rename columns: 'file' for the file identifier, followed by bins     df.columns = [\"file\", \"(0\u20130.01]\", \"(0.01\u20130.05]\", \"(0.05\u20130.5]\", \"(0.01\u20130.5]\"]      # Melt the DataFrame to long format: one row per bin per file     df = pd.melt(df, id_vars=['file'], var_name='bins')      if array:         # Standardize array naming from various vendor-specific labels         df['file'] = df['file'].replace(regex={             'Axiom_PMRA': 'PMRA',             'Axiom_JAPONICA': 'JAPONICA',             'infinium-omni2.5.v1.5': 'OMNI2.5',             'cytosnp-850k-v1.2': 'CYTOSNP',             'global-screening-array-v.3': 'GSA',             'infinium-omni5-v1.2': 'OMNI5',             'Axiom_PMDA': 'PMDA',             'Axiom_UKB_WCSG': 'UKB-WCSG'         })          # Extract array name and population code         df['array'] = [get_array(i) for i in df['file']]         df['pop'] = [i.split('_')[3] for i in df['file']]          # Normalize naming for UKB_WCSG         df['array'] = df['array'].replace(regex={             'UKB-WCSG': 'UKB_WCSG'         })          df['type'] = 'array'     else:         # Extract composite identifier and population for LPS         df['array'] = [get_lps(i) for i in df['file']]         df['pop'] = [i.split('_')[4] for i in df['file']]         df['type'] = 'lps'      return df  In\u00a0[5]: Copied! <pre># Process both LPS and array results\nlps_res = process_perbin(raw_lps_res)\narray_res = process_perbin(raw_arr_res, array=True)\n</pre> # Process both LPS and array results lps_res = process_perbin(raw_lps_res) array_res = process_perbin(raw_arr_res, array=True) In\u00a0[6]: Copied! <pre># Combine both results into a single DataFrame\ndata = pd.concat([lps_res, array_res])\n</pre> # Combine both results into a single DataFrame data = pd.concat([lps_res, array_res]) <p>Show the first few rows of the merged dataset</p> In\u00a0[7]: Copied! <pre>data.head()\n</pre> data.head() Out[7]: file bins value array pop type 0 perbin_chr10_0.5_lps_AFR_snp_wise.acc_cov.txt (0\u20130.01] 0.573309 LPS_0.5 AFR lps 1 perbin_chr10_0.5_lps_AMR_snp_wise.acc_cov.txt (0\u20130.01] 0.725847 LPS_0.5 AMR lps 2 perbin_chr10_0.5_lps_EAS_snp_wise.acc_cov.txt (0\u20130.01] 0.331660 LPS_0.5 EAS lps 3 perbin_chr10_0.5_lps_EUR_snp_wise.acc_cov.txt (0\u20130.01] 0.530053 LPS_0.5 EUR lps 4 perbin_chr10_0.5_lps_SAS_snp_wise.acc_cov.txt (0\u20130.01] 0.400177 LPS_0.5 SAS lps In\u00a0[8]: Copied! <pre>def do_filter_data(data, bins, pop):\n    \"\"\"\n    Filter data by MAF bin and population\n    \"\"\"\n    pick = (data['bins'] == bins) &amp; (data['pop'] == pop)\n    return data[pick]\n</pre> def do_filter_data(data, bins, pop):     \"\"\"     Filter data by MAF bin and population     \"\"\"     pick = (data['bins'] == bins) &amp; (data['pop'] == pop)     return data[pick] In\u00a0[9]: Copied! <pre># Desired array/LPS order for consistent plotting\ndesired_order = ['GSA', \n                'JAPONICA',\n                'UKB_WCSG',\n                'CYTOSNP', \n                'PMRA', \n                'PMDA',\n                'OMNI2.5', \n                'OMNI5',\n                'LPS_0.5',\n                'LPS_0.75',\n                'LPS_1.0',\n                'LPS_1.25',\n                'LPS_1.5',\n                'LPS_2.0'] \n</pre> # Desired array/LPS order for consistent plotting desired_order = ['GSA',                  'JAPONICA',                 'UKB_WCSG',                 'CYTOSNP',                  'PMRA',                  'PMDA',                 'OMNI2.5',                  'OMNI5',                 'LPS_0.5',                 'LPS_0.75',                 'LPS_1.0',                 'LPS_1.25',                 'LPS_1.5',                 'LPS_2.0']  In\u00a0[10]: Copied! <pre>def plot_1(data, bins, pop, axe):\n    \"\"\"\n    Generate a boxplot for one bin/pop combination on the given axis.\n    \"\"\"\n    filterd_data = do_filter_data(data, bins, pop)\n    sns.boxplot(data=filterd_data, \n                y = 'array', \n                x = 'value', \n                hue='type', fill=False,\n                width=0.4,\n                order=desired_order,\n                fliersize= 0.5,\n                orient='h',\n                ax=axe)\n    threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()\n    axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)\n    axe.grid(axis='both', linewidth=0.5)\n    axe.legend().set_visible(False)\n</pre> def plot_1(data, bins, pop, axe):     \"\"\"     Generate a boxplot for one bin/pop combination on the given axis.     \"\"\"     filterd_data = do_filter_data(data, bins, pop)     sns.boxplot(data=filterd_data,                  y = 'array',                  x = 'value',                  hue='type', fill=False,                 width=0.4,                 order=desired_order,                 fliersize= 0.5,                 orient='h',                 ax=axe)     threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()     axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)     axe.grid(axis='both', linewidth=0.5)     axe.legend().set_visible(False) In\u00a0[11]: Copied! <pre>def full_plot(data):\n    \"\"\"\n    Generate a full grid of plots, one per (bin, pop) combination.\n    \"\"\"\n    cols = data['bins'].unique()\n    rows = data['pop'].unique()\n    fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))\n\n    for i, x in enumerate(rows):\n        for j, y in enumerate(cols):\n            plot_1(data, y, x, axes[i,j])\n            \n            # Ticks\n            axes[i,j].set_xlim(0, 1.01)\n            axes[i,j].set_xticks(np.arange(0, 1.01, 0.2))\n            axes[i,j].set_ylabel(\"\")\n            axes[i,j].set_xlabel(\"\")\n            \n            if j != 0:\n                pass\n                \n            if i == 0:\n                # Title\n                axes[i,j].set_title(label=y, \n                            color='white', \n                            bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'), \n                            x=0.5, pad=12,\n                            fontdict={'fontsize':10})\n            if j == 0:\n                axes[i,j].set_ylabel(ylabel=x,\n                                                color='white',\n                                                labelpad = 12,\n                                                rotation = 'horizontal', \n                                                bbox=dict(facecolor='#b3b3b3', \n                                                          edgecolor='white', \n                                                          boxstyle='round,pad=0.6'),\n                                                fontdict={'fontsize':10})\n            \n    for line_num, line_axis in enumerate(axes):\n        for col_num, ax in enumerate(line_axis):\n            if col_num == 0:\n                continue\n            ax.set_ylim(line_axis[0].get_ylim()) # align axes\n            plt.setp(ax.get_yticklabels(), visible=False)\n                \n    \n    fig.tight_layout(rect=[0.02, 0.02, 1, 1])\n    plt.savefig(f'{output_figs_folder}/cov.pdf', dpi=300)\n</pre>  def full_plot(data):     \"\"\"     Generate a full grid of plots, one per (bin, pop) combination.     \"\"\"     cols = data['bins'].unique()     rows = data['pop'].unique()     fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))      for i, x in enumerate(rows):         for j, y in enumerate(cols):             plot_1(data, y, x, axes[i,j])                          # Ticks             axes[i,j].set_xlim(0, 1.01)             axes[i,j].set_xticks(np.arange(0, 1.01, 0.2))             axes[i,j].set_ylabel(\"\")             axes[i,j].set_xlabel(\"\")                          if j != 0:                 pass                              if i == 0:                 # Title                 axes[i,j].set_title(label=y,                              color='white',                              bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'),                              x=0.5, pad=12,                             fontdict={'fontsize':10})             if j == 0:                 axes[i,j].set_ylabel(ylabel=x,                                                 color='white',                                                 labelpad = 12,                                                 rotation = 'horizontal',                                                  bbox=dict(facecolor='#b3b3b3',                                                            edgecolor='white',                                                            boxstyle='round,pad=0.6'),                                                 fontdict={'fontsize':10})                  for line_num, line_axis in enumerate(axes):         for col_num, ax in enumerate(line_axis):             if col_num == 0:                 continue             ax.set_ylim(line_axis[0].get_ylim()) # align axes             plt.setp(ax.get_yticklabels(), visible=False)                           fig.tight_layout(rect=[0.02, 0.02, 1, 1])     plt.savefig(f'{output_figs_folder}/cov.pdf', dpi=300)  In\u00a0[12]: Copied! <pre>full_plot(data)\n</pre> full_plot(data) In\u00a0[13]: Copied! <pre>df = data\n# Compute group-wise mean and standard deviation\nsummary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(\n    mean=(\"value\", \"mean\"),\n    std=(\"value\", \"std\")\n).reset_index()\n\n# Format summary into \"mean \u00b1 std\" string\nsummary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)\n\n# Convert to a wide-format table: each population is a column\nresult = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()\n\n# Ensure arrays are ordered as defined\nresult[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True)\nresult = result.sort_values([\"bins\", \"array\"])\n\n# Export separate CSV files for each bin\nfor bin_val, df_bin in result.groupby(\"bins\"):\n    df_bin = df_bin.drop(columns=[\"bins\"])\n    df_bin = df_bin.rename(columns={'array': 'Array/LPS'})\n    df_bin.to_csv(f\"{output_table_folder}/coverage_summary_bin_{bin_val}.csv\", index=False)\n</pre> df = data # Compute group-wise mean and standard deviation summary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(     mean=(\"value\", \"mean\"),     std=(\"value\", \"std\") ).reset_index()  # Format summary into \"mean \u00b1 std\" string summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)  # Convert to a wide-format table: each population is a column result = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()  # Ensure arrays are ordered as defined result[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True) result = result.sort_values([\"bins\", \"array\"])  # Export separate CSV files for each bin for bin_val, df_bin in result.groupby(\"bins\"):     df_bin = df_bin.drop(columns=[\"bins\"])     df_bin = df_bin.rename(columns={'array': 'Array/LPS'})     df_bin.to_csv(f\"{output_table_folder}/coverage_summary_bin_{bin_val}.csv\", index=False) <p>Outputs</p> <ul> <li>LPS coverage-evaluation tables</li> <li>LPS accurate-evaluation plot</li> </ul>"},{"location":"evaluation/jupyter-files/02.imp_cov/#processing-raw-data","title":"Processing raw data\u00b6","text":""},{"location":"evaluation/jupyter-files/02.imp_cov/#plotting-data","title":"Plotting data\u00b6","text":""},{"location":"evaluation/jupyter-files/02.imp_cov/#data-exporter","title":"Data Exporter\u00b6","text":""},{"location":"evaluation/jupyter-files/03.prs_cor/","title":"PGS Correlation","text":"<p>Requirements</p> <ul> <li>python&gt;=3.11</li> <li>pandas==2.3.0</li> <li>seaborn==0.13.2</li> <li>matplotlib==3.10.3</li> <li>numpy==2.3.1</li> <li>scipy==1.16.0</li> </ul> <p>Input data</p> <ul> <li> Raw PRS scores</li> </ul> In\u00a0[21]: Copied! <pre>## Input variables\nraw_prs_scores = '../../../evaluation/downstream/data/raw_prs_scores'\n\noutput_figs_folder = '../../../evaluation/downstream/out_figs'\noutput_table_folder = '../../../evaluation/downstream/out_tables/prs_cor'\n</pre> ## Input variables raw_prs_scores = '../../../evaluation/downstream/data/raw_prs_scores'  output_figs_folder = '../../../evaluation/downstream/out_figs' output_table_folder = '../../../evaluation/downstream/out_tables/prs_cor' In\u00a0[9]: Copied! <pre>def read_prs(path):\n    \"\"\"\n    Read a polygenic risk score (PRS) file using pandas.\n    \"\"\"\n    return pd.read_table(path, sep='\\s+')\n</pre> def read_prs(path):     \"\"\"     Read a polygenic risk score (PRS) file using pandas.     \"\"\"     return pd.read_table(path, sep='\\s+') In\u00a0[28]: Copied! <pre>def get_cor_array(wgs, array, pop, trait, dir = raw_prs_scores):\n    \"\"\"\n    Compute correlation between WGS PRS and array-based PRS for a given population and trait.\n\n    Parameters:\n    ----------\n    wgs : pd.DataFrame\n        DataFrame containing WGS PRS scores.\n    array : str\n        Name of the array technology (e.g., 'GSA', 'OMNI2.5').\n    pop : str\n        Population identifier (e.g., 'AFR', 'EUR').\n    trait : str\n        Trait name (e.g., 'BMI', 'DIABETES').\n    dir : str\n        Directory path containing the PRS files.\n\n    Returns:\n    -------\n    list of float\n        List of correlation values between corresponding columns in WGS and array PRS.\n    \"\"\"\n    array_path = f\"{dir}/{array}_{pop}_{trait}.all_score\"\n    arr = read_prs(array_path)\n    cols = wgs.columns[2:]\n    res = [wgs[col].corr(arr[col]) for col in cols]\n    return res\n</pre>  def get_cor_array(wgs, array, pop, trait, dir = raw_prs_scores):     \"\"\"     Compute correlation between WGS PRS and array-based PRS for a given population and trait.      Parameters:     ----------     wgs : pd.DataFrame         DataFrame containing WGS PRS scores.     array : str         Name of the array technology (e.g., 'GSA', 'OMNI2.5').     pop : str         Population identifier (e.g., 'AFR', 'EUR').     trait : str         Trait name (e.g., 'BMI', 'DIABETES').     dir : str         Directory path containing the PRS files.      Returns:     -------     list of float         List of correlation values between corresponding columns in WGS and array PRS.     \"\"\"     array_path = f\"{dir}/{array}_{pop}_{trait}.all_score\"     arr = read_prs(array_path)     cols = wgs.columns[2:]     res = [wgs[col].corr(arr[col]) for col in cols]     return res  In\u00a0[29]: Copied! <pre>def get_cor_pop(pop, array_list, trait_list=None, dir = raw_prs_scores):\n    \"\"\"\n    Compute correlation values for all arrays and traits for a specific population.\n\n    Parameters:\n    ----------\n    pop : str\n        Population identifier.\n    array_list : list of str\n        List of array technologies to compare.\n    trait_list : list of str, optional\n        List of traits to analyze. Defaults to ['BMI', 'CAD', 'DIABETES', 'METABOLIC'] if None.\n    dir : str\n        Directory path containing the PRS files.\n\n    Returns:\n    -------\n    pd.DataFrame\n        Long-format DataFrame containing correlations, array type, trait, and population.\n    \"\"\"\n    if trait_list is None:\n        trait_list = ['BMI', 'CAD', 'DIABETES', 'METABOLIC']\n    \n    res = {}\n\n    for trait in trait_list:\n        wgs_path = f\"{dir}/WGS_{pop}_{trait}.all_score\"\n        wgs = read_prs(wgs_path)\n        \n        all_cor = {}\n        for array in array_list:\n            all_cor[array] = get_cor_array(wgs, array, pop, trait)\n        \n        df = pd.DataFrame(all_cor)\n        df = pd.melt(df, var_name='array', value_name='value')\n        df['trait'] = trait\n\n        res[trait] = df\n        \n    df = pd.concat(res, ignore_index=True)\n    df['pop'] = pop\n    return df\n</pre> def get_cor_pop(pop, array_list, trait_list=None, dir = raw_prs_scores):     \"\"\"     Compute correlation values for all arrays and traits for a specific population.      Parameters:     ----------     pop : str         Population identifier.     array_list : list of str         List of array technologies to compare.     trait_list : list of str, optional         List of traits to analyze. Defaults to ['BMI', 'CAD', 'DIABETES', 'METABOLIC'] if None.     dir : str         Directory path containing the PRS files.      Returns:     -------     pd.DataFrame         Long-format DataFrame containing correlations, array type, trait, and population.     \"\"\"     if trait_list is None:         trait_list = ['BMI', 'CAD', 'DIABETES', 'METABOLIC']          res = {}      for trait in trait_list:         wgs_path = f\"{dir}/WGS_{pop}_{trait}.all_score\"         wgs = read_prs(wgs_path)                  all_cor = {}         for array in array_list:             all_cor[array] = get_cor_array(wgs, array, pop, trait)                  df = pd.DataFrame(all_cor)         df = pd.melt(df, var_name='array', value_name='value')         df['trait'] = trait          res[trait] = df              df = pd.concat(res, ignore_index=True)     df['pop'] = pop     return df In\u00a0[35]: Copied! <pre>## Combine array and LPS data\narray_list = ['global-screening-array-v.3X',\n\t'Axiom_JAPONICAX',\n\t'Axiom_UKB_WCSGX',\n\t'Axiom_PMRAX',\n\t'Axiom_PMDAX',\n\t'cytosnp-850k-v1.2X',\n\t'infinium-omni2.5.v1.5X', \n\t'infinium-omni5-v1.2X',\n\t'LPS0.5X',\n\t'LPS0.75X',\n\t'LPS1.0X',\n\t'LPS1.25X',\n\t'LPS1.5X',\n\t'LPS2.0X']\n\ntrait_list = ['HEIGHT', 'BMI', 'DIABETES', 'METABOLIC']\npop_list = [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"]\n\nres_all = {}\n\nfor pop in pop_list:\n    res_all[pop] = get_cor_pop(pop, array_list, trait_list)\n\ndata = pd.concat(res_all.values(), ignore_index=True)\n</pre> ## Combine array and LPS data array_list = ['global-screening-array-v.3X', \t'Axiom_JAPONICAX', \t'Axiom_UKB_WCSGX', \t'Axiom_PMRAX', \t'Axiom_PMDAX', \t'cytosnp-850k-v1.2X', \t'infinium-omni2.5.v1.5X',  \t'infinium-omni5-v1.2X', \t'LPS0.5X', \t'LPS0.75X', \t'LPS1.0X', \t'LPS1.25X', \t'LPS1.5X', \t'LPS2.0X']  trait_list = ['HEIGHT', 'BMI', 'DIABETES', 'METABOLIC'] pop_list = [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"]  res_all = {}  for pop in pop_list:     res_all[pop] = get_cor_pop(pop, array_list, trait_list)  data = pd.concat(res_all.values(), ignore_index=True) In\u00a0[34]: Copied! <pre>## Polising data\ndata.replace({'array': {\n    'Axiom_PMRAX': 'PMRA',\n    'Axiom_JAPONICAX': 'JAPONICA',\n    'infinium-omni2.5.v1.5X': 'OMNI2.5',\n    'cytosnp-850k-v1.2X': 'CYTOSNP',\n    'global-screening-array-v.3X': 'GSA',\n    'infinium-omni5-v1.2X': 'OMNI5',\n    'Axiom_PMDAX': 'PMDA',\n    'Axiom_UKB_WCSGX': 'UKB_WCSG',\n    'LPS0.5X': 'LPS_0.5',\n    'LPS0.75X': 'LPS_0.75',\n    'LPS1.0X': 'LPS_1.0',\n    'LPS1.25X': 'LPS_1.25',\n    'LPS1.5X': 'LPS_1.5',\n    'LPS2.0X': 'LPS_2.0',\n}}, inplace=True)\ndata['type'] = ['lowpass' if 'LPS' in i else 'array' for i in data['array']]\ndata.columns = ['array', 'value', 'bins', 'pop', 'type']\n</pre> ## Polising data data.replace({'array': {     'Axiom_PMRAX': 'PMRA',     'Axiom_JAPONICAX': 'JAPONICA',     'infinium-omni2.5.v1.5X': 'OMNI2.5',     'cytosnp-850k-v1.2X': 'CYTOSNP',     'global-screening-array-v.3X': 'GSA',     'infinium-omni5-v1.2X': 'OMNI5',     'Axiom_PMDAX': 'PMDA',     'Axiom_UKB_WCSGX': 'UKB_WCSG',     'LPS0.5X': 'LPS_0.5',     'LPS0.75X': 'LPS_0.75',     'LPS1.0X': 'LPS_1.0',     'LPS1.25X': 'LPS_1.25',     'LPS1.5X': 'LPS_1.5',     'LPS2.0X': 'LPS_2.0', }}, inplace=True) data['type'] = ['lowpass' if 'LPS' in i else 'array' for i in data['array']] data.columns = ['array', 'value', 'bins', 'pop', 'type'] <p>Show the first few rows of the merged dataset</p> In\u00a0[33]: Copied! <pre>data.head()\n</pre> data.head() Out[33]: array value bins pop type 0 GSA 0.949918 HEIGHT AFR array 1 GSA 0.948261 HEIGHT AFR array 2 GSA 0.949350 HEIGHT AFR array 3 GSA 0.949349 HEIGHT AFR array 4 GSA 0.946509 HEIGHT AFR array In\u00a0[16]: Copied! <pre>def do_filter_data(data, bins, pop):\n    \"\"\"\n    Filter data for a specific trait bin and population.\n\n    Parameters:\n    ----------\n    data : pd.DataFrame\n        Long-format PRS correlation data.\n    bins : str\n        Trait name (used as the 'bins' column).\n    pop : str\n        Population identifier.\n\n    Returns:\n    -------\n    pd.DataFrame\n        Filtered subset of the data.\n    \"\"\"\n    pick = (data['bins'] == bins) &amp; (data['pop'] == pop)\n    return data[pick]\n</pre> def do_filter_data(data, bins, pop):     \"\"\"     Filter data for a specific trait bin and population.      Parameters:     ----------     data : pd.DataFrame         Long-format PRS correlation data.     bins : str         Trait name (used as the 'bins' column).     pop : str         Population identifier.      Returns:     -------     pd.DataFrame         Filtered subset of the data.     \"\"\"     pick = (data['bins'] == bins) &amp; (data['pop'] == pop)     return data[pick] In\u00a0[17]: Copied! <pre>desired_order = ['GSA', \n                'JAPONICA',\n                'UKB_WCSG',\n                'CYTOSNP', \n                'PMRA', \n                'PMDA',\n                'OMNI2.5', \n                'OMNI5',\n                'LPS_0.5',\n                'LPS_0.75',\n                'LPS_1.0',\n                'LPS_1.25',\n                'LPS_1.5',\n                'LPS_2.0'] \n</pre> desired_order = ['GSA',                  'JAPONICA',                 'UKB_WCSG',                 'CYTOSNP',                  'PMRA',                  'PMDA',                 'OMNI2.5',                  'OMNI5',                 'LPS_0.5',                 'LPS_0.75',                 'LPS_1.0',                 'LPS_1.25',                 'LPS_1.5',                 'LPS_2.0']  In\u00a0[18]: Copied! <pre>def plot_1(data, bins, pop, axe):\n    \"\"\"\n    Plot correlation distributions for PRS arrays vs WGS for a specific population and trait.\n\n    Parameters:\n    ----------\n    data : pd.DataFrame\n        Long-format PRS correlation data.\n    bins : str\n        Trait name used for filtering.\n    pop : str\n        Population identifier used for filtering.\n    axe : matplotlib.axes.Axes\n        Axes object to plot on.\n\n    Notes:\n    ------\n    - Uses Seaborn boxplot.\n    - Highlights the median GSA correlation with a vertical red dashed line.\n    - Orders arrays according to predefined `desired_order`.\n    \"\"\"\n    filterd_data = do_filter_data(data, bins, pop)\n    sns.boxplot(data=filterd_data, \n                y = 'array', \n                x = 'value', \n                hue='type', fill=False,\n                width=0.4,\n                order=desired_order,\n                fliersize= 0.5,\n                orient='h',\n                ax=axe)\n    threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()\n    axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)\n    axe.grid(axis='both', linewidth=0.5)\n    axe.legend().set_visible(False)\n</pre> def plot_1(data, bins, pop, axe):     \"\"\"     Plot correlation distributions for PRS arrays vs WGS for a specific population and trait.      Parameters:     ----------     data : pd.DataFrame         Long-format PRS correlation data.     bins : str         Trait name used for filtering.     pop : str         Population identifier used for filtering.     axe : matplotlib.axes.Axes         Axes object to plot on.      Notes:     ------     - Uses Seaborn boxplot.     - Highlights the median GSA correlation with a vertical red dashed line.     - Orders arrays according to predefined `desired_order`.     \"\"\"     filterd_data = do_filter_data(data, bins, pop)     sns.boxplot(data=filterd_data,                  y = 'array',                  x = 'value',                  hue='type', fill=False,                 width=0.4,                 order=desired_order,                 fliersize= 0.5,                 orient='h',                 ax=axe)     threshold = filterd_data[filterd_data['array'] == 'GSA']['value'].median()     axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)     axe.grid(axis='both', linewidth=0.5)     axe.legend().set_visible(False) In\u00a0[22]: Copied! <pre>def full_plot(data):\n    \"\"\"\n    Create a grid of boxplots showing PRS-WGS correlation distributions \n    across traits and populations.\n\n    Parameters:\n    ----------\n    data : pd.DataFrame\n        Long-format DataFrame with the following required columns:\n        - 'array': Name of the array or sequencing method.\n        - 'value': Correlation value between array and WGS PRS.\n        - 'bins': Trait name (used for column grouping).\n        - 'pop': Population identifier (used for row grouping).\n        - 'type': Data type (e.g., 'array', 'lowpass') used for coloring.\n\n    Behavior:\n    --------\n    - Generates subplots with rows = populations, columns = traits.\n    - Each subplot is generated using `plot_1`, which draws a horizontal boxplot.\n    - Common styling includes:\n        - Fixed x-axis limits and ticks.\n        - Trait names as column headers.\n        - Population names as y-axis labels.\n        - Hides redundant y-axis tick labels for visual clarity.\n    - Saves the resulting figure to `prs_corelation.pdf` under `output_figs_folder`.\n\n    \"\"\"\n    cols = data['bins'].unique()\n    rows = data['pop'].unique()\n    fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))\n\n    for i, x in enumerate(rows):\n        for j, y in enumerate(cols):\n            plot_1(data, y, x, axes[i,j])\n            \n            # Ticks\n            axes[i,j].set_xlim(0.9, 1.01)\n            axes[i,j].set_xticks(np.arange(0.9, 1.01, 0.02))\n            axes[i,j].set_ylabel(\"\")\n            axes[i,j].set_xlabel(\"\")\n            \n            if j != 0:\n                pass\n                \n            if i == 0:\n                # Title\n                axes[i,j].set_title(label=y, \n                            color='white', \n                            bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'), \n                            x=0.5, pad=12,\n                            fontdict={'fontsize':10})\n            if j == 0:\n                axes[i,j].set_ylabel(ylabel=x,\n                                                color='white',\n                                                labelpad = 12,\n                                                rotation = 'horizontal', \n                                                bbox=dict(facecolor='#b3b3b3', \n                                                          edgecolor='white', \n                                                          boxstyle='round,pad=0.6'),\n                                                fontdict={'fontsize':10})\n            \n    for line_num, line_axis in enumerate(axes):\n        for col_num, ax in enumerate(line_axis):\n            if col_num == 0:\n                continue\n            ax.set_ylim(line_axis[0].get_ylim()) # align axes\n            plt.setp(ax.get_yticklabels(), visible=False)\n                \n    \n    fig.tight_layout(rect=[0.02, 0.02, 1, 1])\n    plt.savefig(f'{output_figs_folder}/prs_corelation.pdf', dpi=300)\n</pre> def full_plot(data):     \"\"\"     Create a grid of boxplots showing PRS-WGS correlation distributions      across traits and populations.      Parameters:     ----------     data : pd.DataFrame         Long-format DataFrame with the following required columns:         - 'array': Name of the array or sequencing method.         - 'value': Correlation value between array and WGS PRS.         - 'bins': Trait name (used for column grouping).         - 'pop': Population identifier (used for row grouping).         - 'type': Data type (e.g., 'array', 'lowpass') used for coloring.      Behavior:     --------     - Generates subplots with rows = populations, columns = traits.     - Each subplot is generated using `plot_1`, which draws a horizontal boxplot.     - Common styling includes:         - Fixed x-axis limits and ticks.         - Trait names as column headers.         - Population names as y-axis labels.         - Hides redundant y-axis tick labels for visual clarity.     - Saves the resulting figure to `prs_corelation.pdf` under `output_figs_folder`.      \"\"\"     cols = data['bins'].unique()     rows = data['pop'].unique()     fig, axes = plt.subplots(nrows=len(rows), ncols=len(cols), figsize=(11,13))      for i, x in enumerate(rows):         for j, y in enumerate(cols):             plot_1(data, y, x, axes[i,j])                          # Ticks             axes[i,j].set_xlim(0.9, 1.01)             axes[i,j].set_xticks(np.arange(0.9, 1.01, 0.02))             axes[i,j].set_ylabel(\"\")             axes[i,j].set_xlabel(\"\")                          if j != 0:                 pass                              if i == 0:                 # Title                 axes[i,j].set_title(label=y,                              color='white',                              bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'),                              x=0.5, pad=12,                             fontdict={'fontsize':10})             if j == 0:                 axes[i,j].set_ylabel(ylabel=x,                                                 color='white',                                                 labelpad = 12,                                                 rotation = 'horizontal',                                                  bbox=dict(facecolor='#b3b3b3',                                                            edgecolor='white',                                                            boxstyle='round,pad=0.6'),                                                 fontdict={'fontsize':10})                  for line_num, line_axis in enumerate(axes):         for col_num, ax in enumerate(line_axis):             if col_num == 0:                 continue             ax.set_ylim(line_axis[0].get_ylim()) # align axes             plt.setp(ax.get_yticklabels(), visible=False)                           fig.tight_layout(rect=[0.02, 0.02, 1, 1])     plt.savefig(f'{output_figs_folder}/prs_corelation.pdf', dpi=300)  In\u00a0[24]: Copied! <pre>full_plot(data)\n</pre> full_plot(data) In\u00a0[31]: Copied! <pre>df = data\n# Compute group-wise mean and standard deviation\nsummary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(\n    mean=(\"value\", \"mean\"),\n    std=(\"value\", \"std\")\n).reset_index()\n\n# Format summary into \"mean \u00b1 std\" string\nsummary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)\n\n# Convert to a wide-format table: each population is a column\nresult = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()\n\n# Ensure arrays are ordered as defined\nresult[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True)\nresult = result.sort_values([\"bins\", \"array\"])\n\n# Export separate CSV files for each bin\nfor bin_val, df_bin in result.groupby(\"bins\"):\n    df_bin = df_bin.drop(columns=[\"bins\"])\n    df_bin = df_bin.rename(columns={'array': 'Array/LPS'})\n    df_bin.to_csv(f\"{output_table_folder}/prs_correlation_{bin_val}.csv\", index=False)\n</pre> df = data # Compute group-wise mean and standard deviation summary = df.groupby([\"type\", \"bins\", \"array\", \"pop\"]).agg(     mean=(\"value\", \"mean\"),     std=(\"value\", \"std\") ).reset_index()  # Format summary into \"mean \u00b1 std\" string summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)  # Convert to a wide-format table: each population is a column result = summary.pivot(index=[\"bins\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()  # Ensure arrays are ordered as defined result[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True) result = result.sort_values([\"bins\", \"array\"])  # Export separate CSV files for each bin for bin_val, df_bin in result.groupby(\"bins\"):     df_bin = df_bin.drop(columns=[\"bins\"])     df_bin = df_bin.rename(columns={'array': 'Array/LPS'})     df_bin.to_csv(f\"{output_table_folder}/prs_correlation_{bin_val}.csv\", index=False) <p>Outputs</p> <ul> <li>PGS correlation tables</li> <li>PGS correlation plot</li> </ul>"},{"location":"evaluation/jupyter-files/03.prs_cor/#processing-raw-data","title":"Processing raw data\u00b6","text":""},{"location":"evaluation/jupyter-files/03.prs_cor/#plotting-data","title":"Plotting data\u00b6","text":""},{"location":"evaluation/jupyter-files/03.prs_cor/#data-exporter","title":"Data Exporter\u00b6","text":""},{"location":"evaluation/jupyter-files/04.pct_abs_dif/","title":"ADPR","text":"<p>Requirements</p> <ul> <li>python&gt;=3.11</li> <li>pandas==2.3.0</li> <li>seaborn==0.13.2</li> <li>matplotlib==3.10.3</li> <li>numpy==2.3.1</li> <li>scipy==1.16.0</li> </ul> <p>Input data</p> <ul> <li> Percentile PRS scores</li> </ul> In\u00a0[1]: Copied! <pre>## Import Python packages\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> ## Import Python packages import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>## Input variables\nraw_prs_scores = '../../../evaluation/downstream/data/process_prs_scores'\n\noutput_figs_folder = '../../../evaluation/downstream/out_figs/prs_ADPR'\noutput_table_folder = '../../../evaluation/downstream/out_tables/prs_percentile'\n</pre> ## Input variables raw_prs_scores = '../../../evaluation/downstream/data/process_prs_scores'  output_figs_folder = '../../../evaluation/downstream/out_figs/prs_ADPR' output_table_folder = '../../../evaluation/downstream/out_tables/prs_percentile' In\u00a0[3]: Copied! <pre>def process_prs_scores(cutoff = 'Pt_5e-08'):\n    \"\"\"\n    Load and process polygenic risk score (PRS) data from a CSV file.\n\n    Parameters:\n    ----------\n    cutoff : str, optional\n        String indicating the p-value threshold or cutoff used in PRS calculation. \n        This is used to construct the file path (default is 'Pt_5e-08').\n\n    Returns:\n    -------\n    pd.DataFrame\n        A DataFrame where:\n        - 'array' names are harmonized to consistent display names.\n        - A new column 'type' is added indicating whether each entry is from a genotyping 'array' or 'lowpass' sequencing.\n    \"\"\"\n    file_path = f'{raw_prs_scores}/{cutoff}.csv'\n    data = pd.read_csv(file_path)\n    data.replace({'array': {\n        'Axiom_PMRAX': 'PMRA',\n        'Axiom_JAPONICAX': 'JAPONICA',\n        'infinium-omni2.5.v1.5X': 'OMNI2.5',\n        'cytosnp-850k-v1.2X': 'CYTOSNP',\n        'global-screening-array-v.3X': 'GSA',\n        'infinium-omni5-v1.2X': 'OMNI5',\n        'Axiom_PMDAX': 'PMDA',\n        'Axiom_UKB_WCSGX': 'UKB_WCSG',\n        'LPS0.5X': 'LPS_0.5',\n        'LPS0.75X': 'LPS_0.75',\n        'LPS1.0X': 'LPS_1.0',\n        'LPS1.25X': 'LPS_1.25',\n        'LPS1.5X': 'LPS_1.5',\n        'LPS2.0X': 'LPS_2.0',\n    }}, inplace=True)\n    data['type'] = ['lowpass' if 'LPS' in i else 'array' for i in data['array']]\n    return data\n</pre>  def process_prs_scores(cutoff = 'Pt_5e-08'):     \"\"\"     Load and process polygenic risk score (PRS) data from a CSV file.      Parameters:     ----------     cutoff : str, optional         String indicating the p-value threshold or cutoff used in PRS calculation.          This is used to construct the file path (default is 'Pt_5e-08').      Returns:     -------     pd.DataFrame         A DataFrame where:         - 'array' names are harmonized to consistent display names.         - A new column 'type' is added indicating whether each entry is from a genotyping 'array' or 'lowpass' sequencing.     \"\"\"     file_path = f'{raw_prs_scores}/{cutoff}.csv'     data = pd.read_csv(file_path)     data.replace({'array': {         'Axiom_PMRAX': 'PMRA',         'Axiom_JAPONICAX': 'JAPONICA',         'infinium-omni2.5.v1.5X': 'OMNI2.5',         'cytosnp-850k-v1.2X': 'CYTOSNP',         'global-screening-array-v.3X': 'GSA',         'infinium-omni5-v1.2X': 'OMNI5',         'Axiom_PMDAX': 'PMDA',         'Axiom_UKB_WCSGX': 'UKB_WCSG',         'LPS0.5X': 'LPS_0.5',         'LPS0.75X': 'LPS_0.75',         'LPS1.0X': 'LPS_1.0',         'LPS1.25X': 'LPS_1.25',         'LPS1.5X': 'LPS_1.5',         'LPS2.0X': 'LPS_2.0',     }}, inplace=True)     data['type'] = ['lowpass' if 'LPS' in i else 'array' for i in data['array']]     return data In\u00a0[4]: Copied! <pre>def do_filter_data(data, trait, pop):\n    \"\"\"\n    Filter PRS data for a specific trait and population.\n\n    Parameters:\n    ----------\n    data : pd.DataFrame\n        Processed PRS score DataFrame (e.g., output from `process_prs_scores`).\n    trait : str\n        Trait name to filter by (e.g., 'BMI', 'HEIGHT').\n    pop : str\n        Population identifier to filter by (e.g., 'EUR', 'EAS').\n    \"\"\"\n    pick = (data['trait'] == trait) &amp; (data['pop'] == pop)\n    return data[pick]\n</pre> def do_filter_data(data, trait, pop):     \"\"\"     Filter PRS data for a specific trait and population.      Parameters:     ----------     data : pd.DataFrame         Processed PRS score DataFrame (e.g., output from `process_prs_scores`).     trait : str         Trait name to filter by (e.g., 'BMI', 'HEIGHT').     pop : str         Population identifier to filter by (e.g., 'EUR', 'EAS').     \"\"\"     pick = (data['trait'] == trait) &amp; (data['pop'] == pop)     return data[pick] In\u00a0[5]: Copied! <pre>desired_order = ['GSA', \n                'JAPONICA',\n                'UKB_WCSG',\n                'CYTOSNP', \n                'PMRA', \n                'PMDA',\n                'OMNI2.5', \n                'OMNI5',\n                'LPS_0.5',\n                'LPS_0.75',\n                'LPS_1.0',\n                'LPS_1.25',\n                'LPS_1.5',\n                'LPS_2.0'] \n</pre> desired_order = ['GSA',                  'JAPONICA',                 'UKB_WCSG',                 'CYTOSNP',                  'PMRA',                  'PMDA',                 'OMNI2.5',                  'OMNI5',                 'LPS_0.5',                 'LPS_0.75',                 'LPS_1.0',                 'LPS_1.25',                 'LPS_1.5',                 'LPS_2.0']  In\u00a0[6]: Copied! <pre>def plot_1(data, trait, pop, axe):\n    \"\"\"\n    Generate a horizontal boxplot of percent difference in PRS performance \n    for a specific trait and population.\n\n    Parameters:\n    ----------\n    data : pd.DataFrame\n        DataFrame containing PRS percent difference ('pct_dif') and metadata \n        including 'array', 'type', 'trait', and 'pop'.\n    trait : str\n        Trait name to filter and plot (e.g., 'BMI', 'HEIGHT').\n    pop : str\n        Population identifier to filter and plot (e.g., 'EUR', 'AFR').\n    axe : matplotlib.axes.Axes\n        Matplotlib axes object on which the plot is drawn.\n    \"\"\"\n    filterd_data = do_filter_data(data, trait, pop)\n    sns.boxplot(data=filterd_data, \n                y = 'array', \n                x = 'pct_dif', \n                hue='type', fill=False,\n                width=0.4,\n                order=desired_order,\n                fliersize= 0.5,\n                orient='h',\n                ax=axe)\n    threshold = filterd_data[filterd_data['array'] == 'GSA']['pct_dif'].median()\n    axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)\n                \n    # Grid\n    axe.grid(axis='both', linewidth=0.5)\n\n    # Turn off Legend\n    axe.legend().set_visible(False)\n</pre> def plot_1(data, trait, pop, axe):     \"\"\"     Generate a horizontal boxplot of percent difference in PRS performance      for a specific trait and population.      Parameters:     ----------     data : pd.DataFrame         DataFrame containing PRS percent difference ('pct_dif') and metadata          including 'array', 'type', 'trait', and 'pop'.     trait : str         Trait name to filter and plot (e.g., 'BMI', 'HEIGHT').     pop : str         Population identifier to filter and plot (e.g., 'EUR', 'AFR').     axe : matplotlib.axes.Axes         Matplotlib axes object on which the plot is drawn.     \"\"\"     filterd_data = do_filter_data(data, trait, pop)     sns.boxplot(data=filterd_data,                  y = 'array',                  x = 'pct_dif',                  hue='type', fill=False,                 width=0.4,                 order=desired_order,                 fliersize= 0.5,                 orient='h',                 ax=axe)     threshold = filterd_data[filterd_data['array'] == 'GSA']['pct_dif'].median()     axe.axvline(threshold, 0, 10, c='r', linestyle =\"--\", linewidth = 1)                      # Grid     axe.grid(axis='both', linewidth=0.5)      # Turn off Legend     axe.legend().set_visible(False) In\u00a0[7]: Copied! <pre>def full_plot(cutoff):\n    \"\"\"\n    Generate a grid of boxplots showing percent difference in PRS performance\n    across traits and populations, for a given PRS p-value cutoff.\n\n    Parameters:\n    ----------\n    cutoff : str\n        P-value threshold string used to locate the corresponding PRS data file\n        (e.g., 'Pt_5e-08'). The file is expected at `{raw_prs_scores}/{cutoff}.csv`.\n\n    \"\"\"\n    \n    data = process_prs_scores(cutoff)\n    cols = data['trait'].unique()\n    rows = data['pop'].unique()\n    fig, axes = plt.subplots(nrows= len(rows) , ncols =  len(cols), figsize=(11,13))\n\n    for i, x in enumerate(rows):\n        for j, y in enumerate(cols):\n            plot_1(data, y, x, axes[i,j])\n            \n            # Ticks\n            axes[i,j].set_xlim(0, 30.1)\n            axes[i,j].set_xticks(np.arange(0, 30.1, 5))\n            axes[i,j].set_ylabel(\"\")\n            axes[i,j].set_xlabel(\"\")\n                \n            if i == 0:\n                # Title\n                axes[i,j].set_title(label=y, \n                            color='white', \n                            bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'), \n                            x=0.5, pad=12,\n                            fontdict={'fontsize':10})\n            if j == 0:\n                axes[i,j].set_ylabel(ylabel=x,\n                                                color='white',\n                                                labelpad = 12,\n                                                rotation = 'horizontal', \n                                                bbox=dict(facecolor='#b3b3b3', \n                                                          edgecolor='white', \n                                                          boxstyle='round,pad=0.6'),\n                                                fontdict={'fontsize':10})\n    \n    for line_num, line_axis in enumerate(axes):\n        for col_num, ax in enumerate(line_axis):\n            if col_num == 0:\n                continue\n            ax.set_ylim(line_axis[0].get_ylim()) # align axes\n            plt.setp(ax.get_yticklabels(), visible=False)\n            \n            \n    fig.tight_layout(rect=[0.02, 0.02, 1, 1])\n    plt.savefig(f'{output_figs_folder}/{cutoff}.pdf', dpi=300)\n    return data\n    \n</pre> def full_plot(cutoff):     \"\"\"     Generate a grid of boxplots showing percent difference in PRS performance     across traits and populations, for a given PRS p-value cutoff.      Parameters:     ----------     cutoff : str         P-value threshold string used to locate the corresponding PRS data file         (e.g., 'Pt_5e-08'). The file is expected at `{raw_prs_scores}/{cutoff}.csv`.      \"\"\"          data = process_prs_scores(cutoff)     cols = data['trait'].unique()     rows = data['pop'].unique()     fig, axes = plt.subplots(nrows= len(rows) , ncols =  len(cols), figsize=(11,13))      for i, x in enumerate(rows):         for j, y in enumerate(cols):             plot_1(data, y, x, axes[i,j])                          # Ticks             axes[i,j].set_xlim(0, 30.1)             axes[i,j].set_xticks(np.arange(0, 30.1, 5))             axes[i,j].set_ylabel(\"\")             axes[i,j].set_xlabel(\"\")                              if i == 0:                 # Title                 axes[i,j].set_title(label=y,                              color='white',                              bbox=dict(facecolor='#b3b3b3', edgecolor='white', boxstyle='round,pad=0.6'),                              x=0.5, pad=12,                             fontdict={'fontsize':10})             if j == 0:                 axes[i,j].set_ylabel(ylabel=x,                                                 color='white',                                                 labelpad = 12,                                                 rotation = 'horizontal',                                                  bbox=dict(facecolor='#b3b3b3',                                                            edgecolor='white',                                                            boxstyle='round,pad=0.6'),                                                 fontdict={'fontsize':10})          for line_num, line_axis in enumerate(axes):         for col_num, ax in enumerate(line_axis):             if col_num == 0:                 continue             ax.set_ylim(line_axis[0].get_ylim()) # align axes             plt.setp(ax.get_yticklabels(), visible=False)                               fig.tight_layout(rect=[0.02, 0.02, 1, 1])     plt.savefig(f'{output_figs_folder}/{cutoff}.pdf', dpi=300)     return data      In\u00a0[8]: Copied! <pre>all_cutoffs = [\"Pt_5e-08\", \"Pt_1e-07\", \"Pt_1e-06\", \"Pt_1e-05\", \"Pt_0.0001\", \"Pt_0.001\", \"Pt_0.01\", \"Pt_0.1\", \"Pt_0.2\", \"Pt_0.3\", \"Pt_0.5\", \"Pt_1\"]\n</pre> all_cutoffs = [\"Pt_5e-08\", \"Pt_1e-07\", \"Pt_1e-06\", \"Pt_1e-05\", \"Pt_0.0001\", \"Pt_0.001\", \"Pt_0.01\", \"Pt_0.1\", \"Pt_0.2\", \"Pt_0.3\", \"Pt_0.5\", \"Pt_1\"] In\u00a0[9]: Copied! <pre>## i for number of table\ni = 11\nfor cutoff in all_cutoffs:\n    print(f'plotting {cutoff}')\n    df = full_plot(cutoff)\n    summary = df.groupby([\"type\", \"trait\", \"array\", \"pop\"]).agg(\n        mean=(\"pct_dif\", \"mean\"),\n        std=(\"pct_dif\", \"std\")\n    ).reset_index()\n    \n    # Format output\n    summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)\n    \n    # Pivot to table format\n    result = summary.pivot(index=[\"trait\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()\n    \n    # Sort by provided order\n    result[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True)\n    result = result.sort_values([\"trait\", \"array\"])\n    result.to_csv(f\"{output_table_folder}/prs_ADPR_{cutoff}.csv\", index=False)\n    i = i + 1\n</pre> ## i for number of table i = 11 for cutoff in all_cutoffs:     print(f'plotting {cutoff}')     df = full_plot(cutoff)     summary = df.groupby([\"type\", \"trait\", \"array\", \"pop\"]).agg(         mean=(\"pct_dif\", \"mean\"),         std=(\"pct_dif\", \"std\")     ).reset_index()          # Format output     summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.3f} \u00b1 {x['std']:.3f}\", axis=1)          # Pivot to table format     result = summary.pivot(index=[\"trait\", \"array\"], columns=\"pop\", values=\"formatted\").reset_index()          # Sort by provided order     result[\"array\"] = pd.Categorical(result[\"array\"], categories=desired_order, ordered=True)     result = result.sort_values([\"trait\", \"array\"])     result.to_csv(f\"{output_table_folder}/prs_ADPR_{cutoff}.csv\", index=False)     i = i + 1 <pre>plotting Pt_5e-08\nplotting Pt_1e-07\nplotting Pt_1e-06\nplotting Pt_1e-05\nplotting Pt_0.0001\nplotting Pt_0.001\nplotting Pt_0.01\nplotting Pt_0.1\nplotting Pt_0.2\nplotting Pt_0.3\nplotting Pt_0.5\nplotting Pt_1\n</pre> <p>Outputs</p> <ul> <li>PGS ADPR tables</li> <li>PGS ADPR plots</li> </ul>"},{"location":"imputation/array_imputation/","title":"Pseudo-array imputation","text":"<p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> <li>shapeit5 (version==5.1.1)</li> <li>Minimac3 (version==2.0.1)</li> <li>Minimac4 (version==1.0.3)</li> </ul> <p>Input data</p> <ul> <li>Samples list of batch</li> <li>Phasing reference</li> <li>Imputation panel</li> <li>Pseudo array VCFs</li> </ul> Array imputation workflow <p></p>"},{"location":"imputation/array_imputation/#prepare-imputation-reference","title":"Prepare imputation reference","text":"<p>Code</p> <p>This script extracts a reference panel, phases pseudo SNP array data using Shapeit5, and prepares the reference for imputation by indexing it in Minimac3 format</p> <p><pre><code>set -ue\n\n\nCHR=$1                             # Chromosome number (e.g., 1, 2, ..., 22)\nARRAY_NAME=$2                      # Name of the pseudo-array (e.g., array1, array2, ...)    \nBATCH_SAMPLE_LIST=$3               # /path/to/batch_sample_list_file\nPSEUDO_ARRAY_VCF=$4                # /path/to/pseudo_array_vcf_file\nREFERENCE_VCF_FILE=$5              # /path/to/reference_vcf_file.vcf.gz\nPHASING_REFERENCE=$6               # /path/to/phasing_reference_file.vcf.gz\n\n## Extract reference\nbcftools view        -S ^${BATCH_SAMPLE_LIST} ${REFERENCE_VCF_FILE} |\\\nbcftools annotate    --rename-chrs rename_chr.txt \\\n                     -Oz -o ref_chr${CHR}.vcf.gz\n\nbcftools index -f ref_chr${CHR}.vcf.gz\n\n## Phasing\nshapeit5_phase_common_static --input ${PSEUDO_ARRAY_VCF}   \\\n                             --reference ref_chr${CHR}.vcf.gz         \\\n                             --region 4 --map ${PHASING_REFERENCE}   \\\n                             --thread 8                               \\\n                             --output phased_${ARRAY_NAME}_chr${CHR}.bcf\n\n## Indexing by Minimac3\nMinimac3 --refHaps ref_chr${CHR}.vcf.gz   \\\n         --processReference               \\\n         --prefix m3vcf_ref_chr${CHR}     \\\n         --cpus 8 \n</code></pre> rename_chr.txt was used to convert to chromosome numeric format. </p>"},{"location":"imputation/array_imputation/#imputation-process","title":"Imputation process","text":"<p>Code</p> <p>Genotype imputation is performed using Minimac4. The phased BCF file is converted and indexed, imputed against a reference panel, and temporary files are removed upon completion. <pre><code>set -ue\n\nARRAY=$1\nCHR=$2\n\n## Input\nPHASED_BCF=phased_${ARRAY}_chr${CHR}.bcf\nMINIMAC3_INDEX_VCF=m3vcf_ref_chr${CHR}.m3vcf.gz\n\n## Imputation\nbcftools view -Oz -o tem_${ARRAY}_chr${CHR}.vcf.gz ${PHASED_BCF}\nbcftools index -f tem_${ARRAY}_chr${CHR}.vcf.gz\n\n\nminimac4 --refHaps ${MINIMAC3_INDEX_VCF}         \\\n         --ChunkLengthMb 50                      \\\n         --ChunkOverlapMb 5                      \\\n         --haps tem_${ARRAY}_chr${CHR}.vcf.gz    \\\n         --format GT,DS,GP                       \\\n         --prefix imputed_${ARRAY}_chr${CHR}     \\\n         --ignoreDuplicates                      \\\n         --cpus 8                                \\\n         --vcfBuffer 1100\n\nrm tem_${ARRAY}_chr${CHR}.vcf.*\n</code></pre></p> <p>Output data</p> <ul> <li>SNP-array VCF files</li> </ul>"},{"location":"imputation/lps_imputation/","title":"Low-pass imputation","text":"<p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> <li>GLIMPSE2 v2.0.0, commit: 8ce534f, release: 2023-06-29</li> </ul> <p>Input data</p> <ul> <li>Samples list of batch</li> <li>Phasing reference</li> <li>Imputation panel</li> <li>Downsampling VCFs</li> </ul>"},{"location":"imputation/lps_imputation/#low-pass-imputation-process","title":"Low-pass imputation process","text":"Low-pass imputation workflow"},{"location":"imputation/lps_imputation/#prepare-imputation-reference","title":"Prepare imputation reference","text":"<p>Code</p> <p>Make GLIMPSE2 imputation reference data <pre><code>set -ue\n\n## INPUT\nBATCH=$1                # e.g. 1, 2, ..., 10\nREF_FOLDER=$2           # e.g. /path/to/reference_panel_folder\nOUT_DIR=$3              # e.g. /path/to/output_directory\nSAMPLE_LIST=$4          # e.g. /path/to/sample_list.txt\nMAP_DIR=$5              # e.g. /path/to/map_directory\n\n\ngen_ref_batch_chr(){\n    chr=$1\n    in_dir=$2\n    out_dir=$3\n    sample_list=$4\n    map_dir=$5\n\n    mkdir -p $out_dir\n\n    echo filtering reference of ${chr}\n\n    bcftools view \\\n        -S ^$sample_list ${in_dir}/chr${i}.vcf.gz  \\\n        -Oz -o ${out_dir}/chr${i}.vcf.gz\n\n    bcftools index -f ${out_dir}/chr${i}.vcf.gz\n\n    bash buid_ref.sh ${out_dir}/chr${i}.vcf.gz ${chr} ${map_dir}/${chr}.b38.gmap.gz ${out_dir}\n}\n\nfor i in {1..22}\ndo\n    gen_ref_batch_chr chr${i} $REF_FOLDER $OUT_DIR $SAMPLE_LIST $MAP_DIR &amp;\ndone\n\nwait\n</code></pre> build_ref.sh splices raw reference panels (VCF files) to prepare the imputation panel for the GLIMPSE2 imputation process (bin files). </p>"},{"location":"imputation/lps_imputation/#imputation-process","title":"Imputation process","text":"<p>Code</p> <p><pre><code>set -ue\n\n## INPUT\nCHR=$1              # e.g. 1, 2, ..., 22\nOUT=$2              # e.g. chr1_10x_lps_imputed.vcf.gz\nCORES=\"${3:-1}\"     # number of cores to use, default is 1\nREF_FOLDER=${4}     # e.g. /path/to/reference_panel_folder\n\nls *${COV}_lps.bam &gt; run_bam_list.txt\n\nrun_imputation_bam_list.sh     run_bam_list.txt         \\\n                               chr${CHR}                \\\n                               ${OUT}                   \\\n                               ${CORES}                 \\\n                               ${REF_FOLDER}            \\\n</code></pre> Imputation processing on autosomes and ligating using <code>GLIMPSE2</code> (run_imputation_bam_list.sh)</p> <p>Output data</p> <ul> <li>lpWGS VCF files</li> </ul>"},{"location":"processing_data/cross_validation/","title":"Cross-Validation Framework","text":"<ul> <li>10-fold cross-validation is used for selected 2054 samples.</li> <li>Samples are distributed in 10 batches and stratified by superpopulation (EAS, EUR, SAS, AFR, AMR) to ensure balanced representation:<ul> <li>4 batches of 251 samples</li> <li>6 batches of 250 samples</li> </ul> </li> <li>In each fold:<ul> <li>90% of data serves as the reference panel.</li> <li>10% of data serves as the target set for imputation (using to prepare true VCFs and downsampled/psudo-array inputs).</li> </ul> </li> </ul> <p>Output</p> <ul> <li>Samples list of batch</li> <li>2504 samples list</li> <li>Population meta</li> </ul>"},{"location":"processing_data/data_simulation/","title":"Data Simulation","text":"Low-pass sequencing dataPseudo SNP Arrays data <ol> <li> <p>Dat Thanh Nguyen, Trang TH Tran, Mai Hoang Tran, Khai Tran, Duy Pham, Nguyen Thuy Duong, Quan Nguyen, and Nam S Vo. A comprehensive evaluation of polygenic score and genotype imputation performances of human snp arrays in diverse populations. Scientific Reports, 12(1):17556, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"processing_data/data_simulation/#low-pass-sequencing-data","title":"Low-pass sequencing data","text":"<p>Simulated at six coverage levels (0.5x\u20132.0x) from mapped CRAM files, adjusting for a 9% duplication rate and incorporating realistic coverage variability.</p> <p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>wget (version==1.21.2)</li> <li>samtools (version==1.13)</li> <li>python&gt;=3.11</li> <li>pysam==0.22.0 </li> </ul> <p>Input data</p> <ul> <li>URL metadata</li> <li>GRCh38/hg38</li> </ul>"},{"location":"processing_data/data_simulation/#downsampling","title":"Downsampling","text":"Code <p>CRAM files is automatically downloaded, its integrity verified via MD5 checksum, converted to BAM format, downsampled to multiple sequencing depths (0.5\u00d7 to 2.0\u00d7), and indexed\u2014while intermediate files are removed to efficiently manage disk space.</p> <p><pre><code>set -ue\n\n## Input\nCRAM_URL=$1\nSAMPLE_ID=$2\nMD5SUM=$3\n\n## Try 5 times to download cram file\nfor ((i=1; i&lt;=5; i++)); do\n    wget $CRAM_URL -O ${SAMPLE_ID}.cram\n\n    actual_md5=\"$(md5sum ${SAMPLE_ID}.cram | awk '{print $1}')\"\n\n    if [ \"$actual_md5\" == \"$MD5SUM\" ]; then\n        echo \"MD5 sum matches! File downloaded successfully.\"\n        break\n    else\n        echo \"MD5 sum does not match. Retrying...\"\n        rm -f ${SAMPLE_ID}.cram\n        wget $CRAM_URL -O ${SAMPLE_ID}.cram\n    fi\ndone\n\n## Convert CRAM to BAM\nsamtools view -b -T hg38.fa -@ 1 -o ${SAMPLE_ID}.bam ${SAMPLE_ID}.cram\n\n## Release space\nrm -f ${SAMPLE_ID}.cram\n\n## Downsampling\nbam_sampling.py --bam ${SAMPLE_ID}.bam         \\\n                --depth 0.5 0.75 1.0 1.25 1.5 2.0         \\\n                --out ${SAMPLE_ID}_0.5_lps.bam ${SAMPLE_ID}_0.75_lps.bam ${SAMPLE_ID}_1.0_lps.bam ${SAMPLE_ID}_1.25_lps.bam ${SAMPLE_ID}_1.5_lps.bam ${SAMPLE_ID}_2.0_lps.bam   \\\n                --bam_size 853210772\n\n## Release space\nrm ${SAMPLE_ID}.bam\n\n## Index bam file\nsamtools index ${SAMPLE_ID}_0.5_lps.bam ${SAMPLE_ID}_0.75_lps.bam ${SAMPLE_ID}_1.0_lps.bam ${SAMPLE_ID}_1.25_lps.bam ${SAMPLE_ID}_1.5_lps.bam ${SAMPLE_ID}_2.0_lps.bam\n</code></pre> bam_sampling.py was used to downsampling bam files.</p>"},{"location":"processing_data/data_simulation/#pseudo-snp-arrays-data","title":"Pseudo SNP Arrays data","text":"<p>Eight genotyping arrays are simulated using known marker sets and harmonized to hg38. Pseudo-array data is generated by removing phasing information.</p> General information of eight genotyping arrays. Array Name Manufacturer Number of Markers (k) Axiom UK Biobank Array Applied Biosystems 820 Axiom JAPONICA Array Applied Biosystems 667 Axiom Precision Medicine Research Array Applied Biosystems 900 Axiom Precision Medicine Diversity Array Applied Biosystems 901 Infinium Global Screening Array v3.0 Illumina 648 Infinium CytoSNP-850K v1.2 Illumina 2,364 Infinium Omni2.5 v1.5 Array Illumina 4,245 Infinium Omni5 v1.2 Array Illumina 4,245 <p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> </ul> <p>Input data</p> <ul> <li>SNP-array pos data<sup>1</sup></li> <li>Samples list of batch</li> <li>Imputation panel</li> </ul>"},{"location":"processing_data/data_simulation/#create-pseudo-snp-array-data","title":"Create pseudo SNP array data","text":"Code <p>Pseudo SNP array data is created by first extracting a subset of samples from a reference VCF file, renaming chromosomes, and indexing the result. Then, a region-based filter is applied using a position list, phased genotypes are converted to unphased format, and the output is saved as a bgzipped VCF file. <pre><code>set -ue\n\nBATCH_NUM=$1\nCHR=$2\nARRAY=$3\n\n## Input data\nBATCH_SAMPLE_LIST=batch_${BATCH_NUM}.txt\nREFERENCE_VCF=chr${CHR}.vcf.gz\nPOS_FILES=$ARRAY:chr${CHR}.txt\n\n## Extract samples\nbcftools view  -S ${BATCH_SAMPLE_LIST} ${REFERENCE_VCF} |\\\nbcftools annotate   --rename-chrs rename_chr.txt  \\\n                    -Oz -o target_chr${CHR}.vcf.gz\n\nbcftools index -f target_chr${CHR}.vcf.gz\n\n## Create pseudo array data\nget_pseudo_array.sh ${POS_FILES} target_chr${CHR}.vcf.gz ${ARRAY}_chr${CHR}.vcf.gz\n</code></pre> get_pseudo_array.sh filters variants from a VCF using a region list, converts phased genotypes to unphased format, and outputs a bgzipped VCF file.</p>"},{"location":"processing_data/variant_filtering/","title":"Variant Filtering","text":"<p>Requirements</p> <ul> <li>Ubuntu 22.04 (8 CPUs, 32 GB)</li> <li>bcftools (version==1.13)</li> </ul> <p>Input</p> <ul> <li>3202 samples 1KGP<sup>1</sup></li> <li>2504 samples list</li> </ul>"},{"location":"processing_data/variant_filtering/#download-high-coverage-vcf-files","title":"Download high-coverage VCF files","text":"<p>From 1000 Genome Project, we download high-coverage (30X) VCF files containing 3202 samples (folder link).</p> <p>Warning</p> <p>Be sure to verify the MD5 checksums of the VCF files. Due to their large size, file transfers may be prone to interruption or corruption during transmission.</p> <p>Code</p> <p>Code was used to download VCF files containing 3202 samples. <pre><code>set -ue\n\nchr_num=$1\nout_dir=$2\nmd5sum_meta=$3\nmax_trial=10\n\n\nURL_SRC=\"https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20201028_3202_phased\"\n\n# check file exis\nif ! [ -f ${out_dir}/chr${chr_num}_raw.vcf.gz ]; then\n    echo \"download chr${chr_num}_raw.vcf.gz\"\n    wget ${URL_SRC}/CCDG_14151_B01_GRM_WGS_2020-08-05_chr${chr_num}.filtered.shapeit2-duohmm-phased.vcf.gz -O ${out_dir}/chr${chr_num}_raw.vcf.gz\nelse\n    echo \"chr${chr_num}_raw.vcf.gz existed in ${out_dir}\"\nfi\n\n# check md5sum\nmd5sum_ex=`md5sum ${out_dir}/chr${chr_num}_raw.vcf.gz | awk -F\" \" '{print $1}'`\nmd5sum_vcf=`awk -v chr=\"$chr_num\" '$1 ~ (\"chr\" chr \".*\\\\.vcf\\\\.gz$\") { print $3 }' $md5sum_meta`\n\ntrial=1\nwhile true\ndo\n    echo \"md5sum_ex: $md5sum_ex, md5sum: $md5sum_vcf,\"\n    if [ $md5sum_ex == $md5sum_vcf ]; then\n        echo \"match md5sum ${out_dir}/chr${chr_num}_raw.vcf.gz in ${trial} trial\"\n        break\n    else\n        echo \"mismatch md5sum ${out_dir}/chr${chr_num}_raw.vcf.gz in ${trial} trial\"\n    fi\n    echo \"download ref for chr${chr_num}\"\n    wget ${URL_SRC}/CCDG_14151_B01_GRM_WGS_2020-08-05_chr${chr_num}.filtered.shapeit2-duohmm-phased.vcf.gz -O ${out_dir}/chr${chr_num}_raw.vcf.gz\n    md5sum_vcf=`md5sum ${out_dir}/chr${chr_num}_raw.vcf.gz | awk -F\" \" '{print $1}'`\n    trial=$((trial+1))\n    echo \"download chr${chr_num} with $trial trials\"\n    if [[ $trial == $max_trial ]]; then\n        echo \"Max trials with chr${chr_num}\"\n        break\n    fi\n\ndone\n\n# Indexing\nbcftools index ${out_dir}/chr${chr_num}_raw.vcf.gz\n</code></pre> md5sum_meta contains information to verified md5sum of downloaded VCF files (source)</p>"},{"location":"processing_data/variant_filtering/#filtering-variants","title":"Filtering variants","text":"<p>VCF files were filtered to retain only bi-allelic SNPs with an allele count \u2265 2 to reduce noise in imputation and evaluation.</p> <p>Code</p> <pre><code>set -ue\n\nRAW_VCF=$1\nSAMPLE_LIST=$2\nFILTERED_VCF=$3\n\n# Get samples in each batch and filtering to get biallelic variants\nbcftools view \\\n    -S $SAMPLE_LIST $RAW_VCF  \\\n    -m2 -M2 \\\n    -v snps |\nbcftools view \\\n    --exclude 'AC&lt;=2' \\\n    -Oz -o $FILTERED_VCF\n\n# Indexing the filtered VCF\nbcftools index -f $FILTERED_VCF\n</code></pre> <p>Sample list of 2504 selected samples.</p> <p>Output</p> <ul> <li>Raw imputation panel</li> </ul> <ol> <li> <p>Marta Byrska-Bishop, Uday S Evani, Xuefang Zhao, Anna O Basile, Haley J Abel, Allison A Regier, Andr\u00e9 Corvelo, Wayne E Clarke, Rajeeva Musunuri, Kshithija Nagulapalli, and others. High-coverage whole-genome sequencing of the expanded 1000 genomes project cohort including 602 trios. Cell, 185(18):3426\u20133440, 2022.\u00a0\u21a9</p> </li> </ol>"}]}